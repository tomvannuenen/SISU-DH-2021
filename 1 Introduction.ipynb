{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WClw_VPOmwXd"
   },
   "source": [
    "# SISU Digital Humanities: Textual and Language Analysis on Social Media\n",
    "\n",
    "### Session 1: Introduction\n",
    "Created by Tom van Nuenen (tom.van_nuenen@kcl.ac.uk) <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "36-_8ZvxmwXf"
   },
   "source": [
    "# Welcome!\n",
    "\n",
    "In this notebook we will go over some basic operations in Python. If you are familiar with Python, this notebook will cover things you already know."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YUirSNPmwXf"
   },
   "source": [
    "## 1. Working with .txt files\n",
    "\n",
    "Write a small utility function `read_file(filename)` that reads a specified file and simply returns all contents as a single string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOne mild, overcast day in August 1969, a bus came'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opening file - your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "_i6-ZX6mmwXg"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nOne mild, overcast day in August 1969, a bus came'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Opening file with function - your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RT6CadvjmwXl"
   },
   "source": [
    "Now, we are going to create a function `split_sentences` that performs some very simple sentence splitting when passed a text string. Each sentence will be represented as a new string, so the function as a whole returns a list of sentence strings. We assume that any occurrence of either  . or ! or ? marks the end of a sentence.\n",
    "\n",
    "First, we'll create a function called `end_of_sentence_marker` that takes as argument a character and returns True if it is an end-of-sentence marker, otherwise it returns False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "mLFgg9QTmwXl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Define your function here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# these tests should return True if your code is correct\n",
    "print(end_of_sentence_marker(\"?\") == True)\n",
    "print(end_of_sentence_marker(\"a\") == False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BemPS6vLmwXo"
   },
   "source": [
    "An important function we will use is the built in `enumerate`. `enumerate` takes as argument any iterable (a string a list etc.). Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "FOl58CxjmwXp"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 P\n",
      "1 y\n",
      "2 t\n",
      "3 h\n",
      "4 o\n",
      "5 n\n"
     ]
    }
   ],
   "source": [
    "for i, character in enumerate(\"Python\"):\n",
    "    print(i, character)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "onXIjY0SmwXs"
   },
   "source": [
    "As we can see, enumerate allows us to iterate over an iterable and for each element in that iterable, it gives us its corresponding index. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nyEfG_65mwXs"
   },
   "source": [
    "Now we can create our function `split_sentences`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "rqcaAMPmmwXt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nOne mild, overcast day in August 1969, a bus came winding its way along a narrow road at the far end of an island in southern Norway, between gardens and rocks, meadows and woods, up and down dale, around sharp bends, sometimes with trees on both sides, as if through a tunnel, sometimes with the sea straight ahead.',\n",
       " ' It belonged to the Arendal Steamship Company and was, like all its buses, painted in two-tone-light and dark-brown livery.',\n",
       " ' It drove over a bridge, along a bay, signaled right, and drew to a halt.',\n",
       " ' The door opened and out stepped a little family.',\n",
       " ' The father, a tall, slim man in a white shirt and light polyester trousers, was carrying two suitcases.',\n",
       " ' The mother, wearing a beige coat and with a light-blue kerchief covering her long hair, was clutching a stroller in one hand and holding the hand of a small boy in the other.',\n",
       " ' The oily, gray exhaust fumes from the bus hung in the air for a moment as it receded into the distance.',\n",
       " '\\n\\n“It’s quite a way to walk,” the father said.',\n",
       " '\\n\\n“Can you manage, Yngve?',\n",
       " '” the mother said, looking down at the boy, who nodded.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def split_sentences(text):\n",
    "    \"Split a text string into a list of sentences.\"\n",
    "    sentences = []\n",
    "    start = 0\n",
    "    for end, character in enumerate(text):\n",
    "        if end_of_sentence_marker(character):\n",
    "            sentence = text[start: end + 1]\n",
    "            sentences.append(sentence)\n",
    "            start = end + 1\n",
    "    return sentences\n",
    "\n",
    "split = split_sentences(test)\n",
    "split[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ggUeYWSmwXw"
   },
   "source": [
    "Within `split_sentences`, we define a variable 'sentences' in which we store the individual sentences. Next, we define a variable `start` and set it to zero. We're doing this as we need to extract both the start position and the end position of each sentence, and we know that the first sentence will always start at position 0.\n",
    "\n",
    "Next, we use `enumerate` to *loop* over all individual characters in the text. Remember that enumerate returns pairs of indexes and their corresponding elements (here characters). For each character we check whether it is an end-of-sentence marker. If it is, the variable end marks the position in text where a sentence ends. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gVXkXvx8mwXw"
   },
   "source": [
    "There is an easier way to do this, however, which is through NLTK. We will use the `sent_tokenize` package, import it, and run it on our `test` data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ObxTV_7_mwXx"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nOne mild, overcast day in August 1969, a bus came winding its way along a narrow road at the far end of an island in southern Norway, between gardens and rocks, meadows and woods, up and down dale, around sharp bends, sometimes with trees on both sides, as if through a tunnel, sometimes with the sea straight ahead.',\n",
       " 'It belonged to the Arendal Steamship Company and was, like all its buses, painted in two-tone-light and dark-brown livery.',\n",
       " 'It drove over a bridge, along a bay, signaled right, and drew to a halt.',\n",
       " 'The door opened and out stepped a little family.',\n",
       " 'The father, a tall, slim man in a white shirt and light polyester trousers, was carrying two suitcases.']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QklXaMOgmwXz"
   },
   "source": [
    "Finally, let's visualize some of these results. We'll create a new variable called `sentence_length`, assigning an empty list to it. We'll then loop over our `split` variable (which contains all split sentences in our test file) and add the length of each sentence to the `sentence_length` variable (tip: use the built-in `len()` function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ForANRI1mwX0"
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UBdsaP2RmwX3"
   },
   "source": [
    "Finally, we'll import matplotlib and plot `sentence_length`. If you did everything right, the below code should give you a graph of the sentence lengths!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAxAJwvimwX4"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(sentence_length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NgBDYstgmwX6"
   },
   "source": [
    "## 2. Working with Pandas\n",
    "\n",
    "Next, we'll import a .csv into a Pandas dataframe. The data was taken from a forum ran by supporters of former US president Donald Trump.\n",
    "\n",
    "We're just getting started today, but ask yourself already: what kinds of questions could I ask of this data? What kinds of themes, patterns, or regularities might I be interested in exploring?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 759
    },
    "executionInfo": {
     "elapsed": 1565,
     "status": "ok",
     "timestamp": 1594315646796,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "TkI-FuY2mwX6",
    "outputId": "40225fb0-0dc2-4271-e9eb-0833875d2b48",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PFzbshU1l7S5"
   },
   "outputs": [],
   "source": [
    "dfNew = df[:10].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfNew"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gHMTVlnAmwYB"
   },
   "source": [
    "Now, we'll write a function `tokenizer()` that takes as input a string. There are lots of ways to do this, but here's one option.\n",
    "- turn the string into lower case \n",
    "- remove newlines and tabs using `translate`\n",
    "- clean up punctuation using `translate` and `string.punctuation`\n",
    "- put all words in a list after removing digits\n",
    "- return said list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YGvN2GhmwYB"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def tokenizer(text):\n",
    "    '''cleans up and tokenizes input string'''\n",
    "    text = text.lower()\n",
    "    bad_chars = ['\\n', '\\t', '”', '“']\n",
    "    textClean = text.translate(str.maketrans({ch: \" \" for ch in bad_chars}))\n",
    "    table = str.maketrans({ch: None for ch in string.punctuation})\n",
    "    no_punct = (s.translate(table) for s in textClean.split(' ') if s != '')\n",
    "    digits_out = [word for word in no_punct if not word.isdigit()]       \n",
    "    return digits_out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you don't understand all these operations, don't worry! For now, we can do the same (but less verbose) using NLTK. All we need to do is to run the `word_tokenize` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "executionInfo": {
     "elapsed": 1127,
     "status": "ok",
     "timestamp": 1594315040944,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "k40LhROBj1Za",
    "outputId": "8241f7ba-de92-4e4e-ae3a-770085c395ef"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def nltk_tokenizer(text): \n",
    "    text = text.lower()\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MvtXxBUnmwYE"
   },
   "source": [
    "Next, we might want to use our tokenizer on only a subset of our data. Create a new dataframe which only contains the first 20 entries of our DataFrame. Also, remove all rows containing empty values (you can use Pandas' `dropna()` method)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KrK94GYTmwYH"
   },
   "source": [
    "Now let's run one of our tokenizer functions, taking as input the `body` column from your dataframe. Loop over each row of your dataframe, and print out the tokenized `body` of each row to see if it works.\n",
    "Also, notice how the output of our own tokenizer differs from the NLTK one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "executionInfo": {
     "elapsed": 1016,
     "status": "ok",
     "timestamp": 1594316332749,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "_TApMXYHAMnR",
    "outputId": "468f41be-8db1-4bda-e502-803268189407",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4a2w-4BvmwYJ"
   },
   "source": [
    "Next, we'll create the type-token ratio for each user in our df, to see whose language is the most 'complex'. First, we'll create a function for you that computes the TTR (see if you understand how it works!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8lo1Gz5hmwYJ"
   },
   "outputs": [],
   "source": [
    "def typeTokenRatio(tokens): \n",
    "    numTokens = len(tokens)\n",
    "    numTypes = len(set(tokens))\n",
    "    return numTypes/numTokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lPddUWELmwYM"
   },
   "source": [
    "Finally, loop over the 'body' column of each row in your df again. This time, within the loop, create a variable `tokens` and assign to it the output of your tokenizer function. Then, print the output of the `typeTokenRatio` function, which you run on `tokens`.\n",
    "\n",
    "If things go well, you'll see the TTR for each of the 20 posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xghYJeQ1mwYM",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# your code here\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "znmgu5ncmwYO"
   },
   "source": [
    "We see that some posts have a TTR of 1, meaning all words are unique. TTR does not tell us much here, as these are all short posts. Still, it is one way of finding more unique posts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qdMGp8AomwYP"
   },
   "source": [
    "## Most-used words\n",
    "\n",
    "Finally, we'll write a short program that tells you the *10 most-used words* for a single user comment in the DataFrame. We will use the `Counter` class from the `collections` module. If you don't know the answer: the Internet is your friend!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SaA19lVKmwYP"
   },
   "outputs": [],
   "source": [
    "# Your code here\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "5YUirSNPmwXf"
   ],
   "name": "1 Introduction.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
