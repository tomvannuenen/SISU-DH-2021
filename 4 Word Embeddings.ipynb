{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-Uds2RtAZOog"
   },
   "source": [
    "# SISU Digital Humanities: Textual and Language Analysis on Social Media<br />\n",
    "### Session 4: Word Embeddings\n",
    "Created by Tom van Nuenen (tom.van_nuenen@kcl.ac.uk) <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MfjNP33MZOoi"
   },
   "source": [
    "# Word Embeddings\n",
    "\n",
    "Today, we'll have a look at word embeddings using Gensim's `word2vec` and `doc2vec` methods. \n",
    "\n",
    "The goal of word vector embedding models is to learn dense, numerical vector representations for each term in a corpus vocabulary. If successful, the vectors for each term encode information about the meaning or concept the term represents, as well as the relationship between it and other terms in the vocabulary. Word vector models are  fully unsupervised: they learn all of these meanings and relationships without any advance knowledge.\n",
    "\n",
    "After working through today's notebook, you'll be able to:\n",
    "\n",
    "1. Use Gensim's word2vec method to create word vectors for a corpus;\n",
    "2. Use these word vectors to reflect on implicit binaries and normativities in your data;\n",
    "3. Visualize topic models using K-means clustering and t-SNE.\n",
    "\n",
    "**Note: I stronly encourage you to use your own dataset to start figuring out semantic patterns that you are interested in!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3273,
     "status": "ok",
     "timestamp": 1596036431956,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "kRNNPkOdZOoj",
    "outputId": "c3f16306-08e1-43b9-9cac-6372cec26558"
   },
   "outputs": [],
   "source": [
    "# General\n",
    "from pprint import pprint\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "import string\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import smart_open\n",
    "import multiprocessing \n",
    "from time import time  # To time our operations\n",
    "from collections import defaultdict  # For word frequency\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from nltk import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Spacy\n",
    "import spacy \n",
    "\n",
    "# Plotting\n",
    "from bokeh.plotting import figure, show, output_notebook\n",
    "from bokeh.models import HoverTool, ColumnDataSource, value\n",
    "\n",
    "# Clustering \n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Suppressing warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wDeidiecZOop"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "We'll start by cleaning up our data a bit. Let's load it up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1042,
     "status": "ok",
     "timestamp": 1596036443476,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "1cY6-yUcZOoq"
   },
   "outputs": [],
   "source": [
    "# load into df\n",
    "df_com = pd.read_csv(\"data/TRP-comments.csv\", lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2006,
     "status": "ok",
     "timestamp": 1596036447373,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "yn3XOMNrZOo1"
   },
   "outputs": [],
   "source": [
    "# Get rid of empty values and reset index\n",
    "df_com = df_com[~df_com['body'].isin(['[removed]', '[deleted]' ])].dropna(subset=['body']).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wLs_XcxbDHzw"
   },
   "source": [
    "Let's create a small function that cleans up our text by removing all escape-tabs and escape-newlines, as well as all non symbol characters (except for the dot). It also normalizes spaces to a single character and removes leading and trailing spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1384,
     "status": "ok",
     "timestamp": 1596036453127,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "JEdJGFrnY_nG"
   },
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Normalize tabs and remove newlines\n",
    "    no_tabs = text.replace('\\t', ' ').replace('\\n', '');\n",
    "    # Remove all characters except A-Z and a dot.\n",
    "    alphas_only = re.sub(\"[^a-zA-Z\\.]\", \" \", no_tabs);\n",
    "    # Normalize spaces to 1\n",
    "    multi_spaces = re.sub(\" +\", \" \", alphas_only);\n",
    "    # Strip trailing and leading spaces\n",
    "    no_spaces = multi_spaces.strip();\n",
    "    return no_spaces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yj_uE4xZ-bdY"
   },
   "source": [
    "We can now use the Pandas `.apply` method, allowing us to apply a function along an axis of the DataFrame. We'll use a lambda function: a small stand-in function that can take arguments, but only one expression. This is what a lambda looks like. Do you see how it works?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1392,
     "status": "ok",
     "timestamp": 1596036453562,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "UmgmLpMd9fF_"
   },
   "outputs": [],
   "source": [
    "df_com[\"body_clean\"] = df_com[\"body\"].apply(lambda x: clean_text(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Ok7Ham8-wa9"
   },
   "source": [
    "We now have an additional column in our DataFrame with cleaned up text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 510
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1008,
     "status": "ok",
     "timestamp": 1596036455593,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "7u2Z_-yO-GHJ",
    "outputId": "c0badac3-ed07-48e7-9aa9-7ae690170cc3"
   },
   "outputs": [],
   "source": [
    "df_com.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KntxWVim-1K_"
   },
   "source": [
    "Let's turn it into a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1308,
     "status": "ok",
     "timestamp": 1596036458863,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "FLjNJaCd3D17"
   },
   "outputs": [],
   "source": [
    "text_li = df_com['body_clean'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fYwCcPjN-7id"
   },
   "source": [
    "Next, we'll create a function that uses NLTK's `sent_tokenize()` method. This tokenizer splits our texts into sentences, which in turn are split into tokens. We'll also remove stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1332,
     "status": "ok",
     "timestamp": 1596036461949,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "xEOgByps3aVx"
   },
   "outputs": [],
   "source": [
    "def sentence_tokenize(text):\n",
    "    sentence_doc = sent_tokenize(text)\n",
    "    sentences = [gensim.utils.simple_preprocess(str(doc), deacc=True) for doc in sentence_doc]  # deacc=True removes punctuations\n",
    "    stop = set(stopwords.words('english') + ['’', '“', '”', 'nbsp', 'http'])\n",
    "    no_stop = [[word for word in sentence if word not in stop] for sentence in sentences]\n",
    "    return no_stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7818,
     "status": "ok",
     "timestamp": 1596036478305,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "k-8QFdsy3bPO"
   },
   "outputs": [],
   "source": [
    "com_sent_li = [sentence_tokenize(text) for text in text_li]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0svSRqk7v7Z"
   },
   "source": [
    "Note that we now have a list (of comments) of lists (sentences) of lists (tokens). Let's index the first token of the first sentence of the first comment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1411,
     "status": "ok",
     "timestamp": 1596036481829,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "EgdIVhrt4VxE",
    "outputId": "dca9339e-1e89-46a7-f03c-c5de96a8d178"
   },
   "outputs": [],
   "source": [
    "com_sent_li[0][0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wmiEo5_66t16"
   },
   "source": [
    "We actually don't need the comment-level demarcation for the rest of our analysis. We can *flatten* our `com_sent_li` object to do so – this way, we create a list (of sentences) of lists (tokens)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1542,
     "status": "ok",
     "timestamp": 1596036484338,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "gVb8-KIU7_l-"
   },
   "outputs": [],
   "source": [
    "sent_li = []\n",
    "for sentence in com_sent_li:\n",
    "    for tokens in sentence:\n",
    "        sent_li.append(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5q9nQ2K08QGX"
   },
   "source": [
    "Writing the same in a list comprehension looks like this, by the way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1097,
     "status": "ok",
     "timestamp": 1596036487737,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "aPfdxQNr7PMi"
   },
   "outputs": [],
   "source": [
    "sent_li = [tokens for sentence in com_sent_li for tokens in sentence]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "p33QbO4G8kGs"
   },
   "source": [
    "Next, let's create a trigrams model using Gensim's `Phrases` and `Phraser` classes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 12502,
     "status": "ok",
     "timestamp": 1596036500687,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "QBZvKWy93ZWm",
    "outputId": "ed9ba9dc-9f11-4ca9-f9ab-9dc533991a13"
   },
   "outputs": [],
   "source": [
    "bigram = Phrases(sent_li, min_count=5, threshold=80)\n",
    "trigram = Phrases(bigram[sent_li], threshold=80)  \n",
    "bigram_mod = Phraser(bigram)\n",
    "trigram_mod = Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J65kFo458nJH"
   },
   "source": [
    "And let's run that model over our list of lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4199,
     "status": "ok",
     "timestamp": 1596036506357,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "gdZ5MqsG8mfF"
   },
   "outputs": [],
   "source": [
    "trigrams = [trigram_mod[bigram_mod[sentence]] for sentence in sent_li]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wk2mxX1HZOpk"
   },
   "source": [
    "## Word2Vec\n",
    "\n",
    "Let's create our word embeddings model. Its input is a text corpus (split up in sentences) and its output is a set of \"vectors\" in N dimensions. It allows us to group the vectors of similar words together in vectorspace. We can then reduce the dimensionality to visualize the results in a way humans can understand (such as in a 2-dimensional space), or to perform linear algebra in order to find how words are related.\n",
    "\n",
    "Word2vec is one example of a word embeddings model. It learns by taking words and their contexts (e.g. sentences) into account, and can then try to predict other words. Given enough data, usage and contexts, word2vec can make accurate guesses about a word’s meaning based on its appearances. Those guesses can be used to establish a word’s association with other words (e.g. “man” is to “boy” what “woman” is to “girl”), or cluster documents and classify them by topic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ykxhE2K3Bqfj"
   },
   "source": [
    "### How many cores?\n",
    "Word2Vec can work using independent threads doing simultaneous training. In general, you'll never want to use more workers than the number of CPU cores you have in your machine. So let's check out how many you have.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1587,
     "status": "ok",
     "timestamp": 1596036532568,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "LN1nsJ7cO_oT",
    "outputId": "a3cca3ef-0756-4da7-d33d-6fa8e7bd28bb"
   },
   "outputs": [],
   "source": [
    "cores = multiprocessing.cpu_count() # Count the number of cores in a computer\n",
    "cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hIt-VOPQCQiP"
   },
   "source": [
    "We now instantiate and train our Word2Vec model, using the parameters below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39180,
     "status": "ok",
     "timestamp": 1596037818439,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "sQq117-pZOpl"
   },
   "outputs": [],
   "source": [
    "num_features = 500        # Word vector dimensionality (how many features each word will be given)\n",
    "min_word_count = 2        # Minimum word count to be taken into account\n",
    "num_workers = cores       # Number of threads to run in parallel (equal to your amount of cores)\n",
    "context = 10              # Context window size\n",
    "downsampling = 0 #1e-2    # Downsample setting for frequent words\n",
    "seed_n = 1                # Seed for the random number generator (to create reproducible results) \n",
    "sg_n = 1                  # Skip-gram = 1, CBOW = 0\n",
    "\n",
    "model = Word2Vec(trigrams, workers=num_workers, \\\n",
    "            size=num_features, min_count = min_word_count, \\\n",
    "            window = context, sample = downsampling, seed=seed_n, sg=sg_n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xrNV3eYeZOpo"
   },
   "source": [
    "That was it! We can save this model in a Gensim object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39251,
     "status": "ok",
     "timestamp": 1596035659089,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "uOzf_HW4ZOpo",
    "outputId": "878c1a0a-1750-4e61-b57f-eb78e67ea9a1"
   },
   "outputs": [],
   "source": [
    "model.save(\"word2vec.vec\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 71
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 39645,
     "status": "ok",
     "timestamp": 1596035659717,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "4l3NDavhZOpr",
    "outputId": "cfae7151-f7bd-4df9-fc86-b41067992e84"
   },
   "outputs": [],
   "source": [
    "model = Word2Vec.load(\"word2vec.vec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wWQC4S9IZOpu"
   },
   "source": [
    "How many terms are in our vocabulary?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1197,
     "status": "ok",
     "timestamp": 1596037853832,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "SegRmcybZOpw",
    "outputId": "c2622fb6-d414-49d5-c072-9e0c2719f40b"
   },
   "outputs": [],
   "source": [
    "print('{:,} terms in the vocabulary.'.format(len(model.wv.vocab)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xw4th__0ZOp0"
   },
   "source": [
    "### Getting related terms\n",
    "\n",
    "With the information in our word embeddings model, we can try to find similarities between words that interest us (i.e. words that have a similar vector). Let's create a function that retrieves related terms to some input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1492,
     "status": "ok",
     "timestamp": 1596037858997,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "v1HVouFqZOp0"
   },
   "outputs": [],
   "source": [
    "def get_related_terms(token, topn=20):\n",
    "    \"\"\"\n",
    "    look up the topn most similar terms to token and print them as a formatted list\n",
    "    \"\"\"\n",
    "\n",
    "    for word, similarity in model.most_similar(positive=[token], topn=topn):\n",
    "        print(word, round(similarity, 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 411
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1356,
     "status": "ok",
     "timestamp": 1596037862238,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "CDyV-ntGZOp3",
    "outputId": "b27dcc9f-ebc7-43cc-f301-f5120a0f7554"
   },
   "outputs": [],
   "source": [
    "get_related_terms(u'woman')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LAF3aRsHZOp6"
   },
   "source": [
    "### Word algebra\n",
    "\n",
    "Word algebra, also known as analogy completion, means doing math with words (like the famous example \"king - man + woman = queen\". The core idea is that once words are represented as numerical vectors, you can do math with them. The mathematical procedure works as follows:\n",
    "\n",
    "1. Provide a set of words or phrases you want to add or subtract.\n",
    "2. Look up the vectors that represent those terms in the word vector model.\n",
    "3. Add and subtract those vectors to produce a new, combined vector.\n",
    "4. Look up the most similar vector(s) to this new, combined vector via cosine similarity.\n",
    "5. Return the word(s) associated with the similar vector(s).\n",
    "\n",
    "Let's try it out. We'll create a function that does this for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1516,
     "status": "ok",
     "timestamp": 1596037867247,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "X8RsoWBAZOp7"
   },
   "outputs": [],
   "source": [
    "def word_algebra(add=[], subtract=[], topn=1):\n",
    "    \"\"\"\n",
    "    combine the vectors associated with the words provided\n",
    "    in add= and subtract=, look up the topn most similar\n",
    "    terms to the combined vector, and print the result(s)\n",
    "    \"\"\"\n",
    "    answers = model.most_similar(positive=add, negative=subtract, topn=topn)\n",
    "    \n",
    "    for term, similarity in answers:\n",
    "        print(term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1733,
     "status": "ok",
     "timestamp": 1596037867714,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "hXhZB8SOZOp9",
    "outputId": "c49eb164-300c-4878-e587-ae94d1d3029c",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_algebra(add=['game','dating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1479,
     "status": "ok",
     "timestamp": 1596037867715,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "ho-F51R3ZOqB",
    "outputId": "d129dd45-c615-4551-9779-3c6b7af9ff05"
   },
   "outputs": [],
   "source": [
    "word_algebra(add=['game', 'dating'], subtract=['beta'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5p4obNSvZOqG"
   },
   "source": [
    "## Word Vector Visualization with t-SNE\n",
    "\n",
    "t-Distributed Stochastic Neighbor Embedding, or t-SNE, is a dimensionality reduction technique to assist with visualizing high-dimensional datasets. It attempts to map high-dimensional data onto a low two- or three-dimensional representation. It tries to keep the relative distances between points as closely as possible in both high-dimensional and low-dimensional space.\n",
    "\n",
    "Scikit-learn provides a convenient implementation of the t-SNE algorithm with its `TSNE` class.\n",
    "\n",
    "Our input for t-SNE will be the DataFrame of word vectors we created before. Let's first drop some stopwords, and\n",
    "take only the 5,000 most frequent terms in the vocabulary for time's sake.\n",
    "\n",
    "First, we need to create a DataFrame with the terms as the row labels, and the 100 dimensions of the word vector model as the columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 253
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1844,
     "status": "ok",
     "timestamp": 1596037883761,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "-Tusa5xMZOqH",
    "outputId": "6b25fae7-af0e-41ad-fa77-a29866d70709"
   },
   "outputs": [],
   "source": [
    "# build a list of the terms, integer indices, and term counts from the word2vec model vocabulary\n",
    "ordered_vocab = [(term, voc.index, voc.count) for term, voc in model.wv.vocab.items()]\n",
    "\n",
    "# sort by the term counts, so the most common terms appear first\n",
    "ordered_vocab.sort(key = lambda x: x[2])  \n",
    "\n",
    "# unzip the terms, integer indices, and counts into separate lists\n",
    "ordered_terms, term_indices, term_counts = zip(*ordered_vocab)\n",
    "\n",
    "# create a DataFrame with the vectors as data, and the terms as row labels\n",
    "word_vectors = pd.DataFrame(model.wv.syn0norm[term_indices, :], index=ordered_terms)\n",
    "\n",
    "word_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mSeCHT9HZOqK"
   },
   "outputs": [],
   "source": [
    "tsne = TSNE()\n",
    "tsne_vectors = tsne.fit_transform(word_vectors.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kZTyvI2gZOqO"
   },
   "outputs": [],
   "source": [
    "tsne_vectors = pd.DataFrame(tsne_vectors,\n",
    "                            index=pd.Index(word_vectors.index),\n",
    "                            columns=['x_coord', 'y_coord'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gO4nrS52ZOqQ"
   },
   "outputs": [],
   "source": [
    "tsne_vectors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IyDMY7rsZOqT"
   },
   "outputs": [],
   "source": [
    "tsne_vectors['word'] = tsne_vectors.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VgpLvkq9ZOqV"
   },
   "outputs": [],
   "source": [
    "output_notebook()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Dl8fu0UNZOqY"
   },
   "outputs": [],
   "source": [
    "# add our DataFrame as a ColumnDataSource for Bokeh\n",
    "plot_data = ColumnDataSource(tsne_vectors)\n",
    "\n",
    "# create the plot and configure the\n",
    "# title, dimensions, and tools\n",
    "tsne_plot = figure(title='t-SNE Word Embeddings',\n",
    "                   plot_width = 800,\n",
    "                   plot_height = 800,\n",
    "                   tools= ('pan, wheel_zoom, box_zoom, box_select, reset, reset'),\n",
    "                   active_scroll='wheel_zoom')\n",
    "\n",
    "# add a hover tool to display words on roll-over\n",
    "tsne_plot.add_tools(HoverTool(tooltips = '@word'))\n",
    "\n",
    "# draw the words as circles on the plot\n",
    "tsne_plot.circle('x_coord', 'y_coord', source=plot_data,\n",
    "                 color='blue', line_alpha=0.2, fill_alpha=0.1,\n",
    "                 size=10, hover_line_color='black')\n",
    "\n",
    "# configure visual elements of the plot\n",
    "tsne_plot.title.text_font_size = value('14pt')\n",
    "tsne_plot.xaxis.visible = False\n",
    "tsne_plot.yaxis.visible = False\n",
    "tsne_plot.grid.grid_line_color = None\n",
    "tsne_plot.outline_line_color = None\n",
    "\n",
    "# show the plot\n",
    "show(tsne_plot);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L6Y4eXciZOqb"
   },
   "source": [
    "## K-means clustering\n",
    "Another thing we can do is cluster the words using K-Means clustering. \n",
    "\n",
    "K-Means clustering aims to partition N observations into K clusters in which each observation belongs to the cluster with the nearest mean (called the \"cluster centre\"), which serves as a prototype of the cluster.\n",
    "\n",
    "Since our words are all represented as vectors, applying K-Means is easy to do since the clustering algorithm will simply look at differences between vectors (and centers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1284,
     "status": "ok",
     "timestamp": 1596036750897,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "rJodQkitZOqb"
   },
   "outputs": [],
   "source": [
    "def clustering_on_wordvecs(word_vectors, num_clusters):\n",
    "    # Initalize a k-means object and use it to extract centroids\n",
    "    kmeans_clustering = KMeans(n_clusters = num_clusters, init='k-means++');\n",
    "    idx = kmeans_clustering.fit_predict(word_vectors);\n",
    "    \n",
    "    return kmeans_clustering.cluster_centers_, idx;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1301,
     "status": "ok",
     "timestamp": 1596036752843,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "qPRtxyZfZOqd"
   },
   "outputs": [],
   "source": [
    "Z = model.wv.syn0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 21475,
     "status": "ok",
     "timestamp": 1596036775406,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "Em2CvpgRZOqf"
   },
   "outputs": [],
   "source": [
    "centers, clusters = clustering_on_wordvecs(Z, 10);\n",
    "centroid_map = dict(zip(model.wv.index2word, clusters));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "15_523YKZOqj"
   },
   "source": [
    "Next, we get words in each cluster that are closest to the cluster center. To do this, we initialize a KDTree on the word vectors, and query it for the Top K words on each cluster center. Using the Index 2 word dictionary, we than correspond each word vector back to it’s original word representation and add them to a dataframe for easier printing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1144,
     "status": "ok",
     "timestamp": 1596036783240,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "O9aD76xcZOqk"
   },
   "outputs": [],
   "source": [
    "def get_top_words(index2word, k, centers, wordvecs):\n",
    "    tree = KDTree(wordvecs);\n",
    "    # Use closest points for each cluster center to query closest 20 points to it\n",
    "    closest_points = [tree.query(np.reshape(x, (1, -1)), k=k) for x in centers];\n",
    "    closest_words_idxs = [x[1] for x in closest_points];\n",
    "    # Query Word Index  for each position in the above array, and added to a Dictionary\n",
    "    closest_words = {};\n",
    "    for i in range(0, len(closest_words_idxs)):\n",
    "        closest_words['Cluster #' + str(i)] = [index2word[j] for j in closest_words_idxs[i][0]]\n",
    "    # Create DataFrame from dictionary\n",
    "    df = pd.DataFrame(closest_words);\n",
    "    df.index = df.index+1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OtBFZL_HZOqm"
   },
   "source": [
    "Let’s get the top words and print the first 20 in each cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2179,
     "status": "ok",
     "timestamp": 1596036787730,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "SwzjSQJlZOqn"
   },
   "outputs": [],
   "source": [
    "top_words = get_top_words(model.wv.index2word, 5000, centers, Z);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1942,
     "status": "ok",
     "timestamp": 1596036787731,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "tI19jNzsZOqq",
    "outputId": "0525155d-011f-4001-f7cd-e87eb67a8948",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "top_words[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iwP_v_AGX88e"
   },
   "source": [
    "## **Doc2Vec**\n",
    "\n",
    "You might be interested in the relationship between comments or posts, instead of single words. You can do this using tf-idf, as you saw in week 2. But word embeddings offer another unsupervised approach to find similarities.\n",
    "\n",
    "Let's read our data again. We’ll need to associate a tag/number with each document of the training corpus. In our case, the tag is simply a zero-based line number.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2043,
     "status": "ok",
     "timestamp": 1596037257163,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "zF3SWaiOX7DV"
   },
   "outputs": [],
   "source": [
    "def read_text(inp):\n",
    "    for i, line in enumerate(inp):\n",
    "        tokens = gensim.utils.simple_preprocess(line)\n",
    "        yield gensim.models.doc2vec.TaggedDocument(tokens, [i])\n",
    "\n",
    "comments = list(read_text(df_com['body_clean']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nWb9ViBzbxMP"
   },
   "source": [
    "Instantiate a Doc2Vec model with a vector size with 50 dimensions and iterating over the training corpus 40 times. We set the minimum word count to 2 in order to discard words with very few occurrences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1475,
     "status": "ok",
     "timestamp": 1596037380724,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "rXJFIbGIbkCq"
   },
   "outputs": [],
   "source": [
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jSYSv_yibogQ"
   },
   "source": [
    "Build the vocabulary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 7183,
     "status": "ok",
     "timestamp": 1596037386864,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "pp0ISnCrbn5R"
   },
   "outputs": [],
   "source": [
    "model.build_vocab(comments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xspWe0XIcHbp"
   },
   "source": [
    "Train the model on the corpus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 99532,
     "status": "ok",
     "timestamp": 1596037504178,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "72vHdeQmcINQ"
   },
   "outputs": [],
   "source": [
    "model.train(comments, total_examples=model.corpus_count, epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q7iAdN6pcvds"
   },
   "source": [
    "To assess our new model, we’ll first infer new vectors for each document of the training corpus, compare the inferred vectors with the training corpus, and then returning the rank of the document based on self-similarity. Basically, we’re pretending as if the training corpus is some new unseen data and then seeing how they compare with the trained model. The expectation is that we’ve likely overfit our model (i.e., all of the ranks will be less than 2) and so we should be able to find similar documents very easily. Additionally, we’ll keep track of the second ranks for a comparison of less similar documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 429
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 350784,
     "status": "error",
     "timestamp": 1596037764255,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "UUwjxPHWcsPz",
    "outputId": "0762e492-fea3-4220-ca48-19c5eace95b8"
   },
   "outputs": [],
   "source": [
    "ranks = []\n",
    "second_ranks = []\n",
    "for doc_id in range(len(comments)):\n",
    "    inferred_vector = model.infer_vector(comments[doc_id].words)\n",
    "    sims = model.docvecs.most_similar([inferred_vector], topn=len(model.docvecs))\n",
    "    rank = [docid for docid, sim in sims].index(doc_id)\n",
    "    ranks.append(rank)\n",
    "\n",
    "    second_ranks.append(sims[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9242eLY5dATL"
   },
   "source": [
    "Let's find some similar comments!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cRi45HBOc2q6"
   },
   "outputs": [],
   "source": [
    "print('Comment ({}): «{}»\\n'.format(doc_id, ' '.join(comments[doc_id].words)))\n",
    "print(u'SIMILAR/DISSIMILAR COMMENTS PER MODEL %s:\\n' % model)\n",
    "for label, index in [('MOST', 0), ('SECOND-MOST', 1), ('MEDIAN', len(sims)//2), ('LEAST', len(sims) - 1)]:\n",
    "    print(u'%s %s: «%s»\\n' % (label, sims[index], ' '.join(comments[sims[index][0]].words)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ip0ee9CydIHA"
   },
   "source": [
    "And let's automate that a bit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CTVjJZPndEBl"
   },
   "outputs": [],
   "source": [
    "# Pick a random document from the corpus and infer a vector from the model\n",
    "import random\n",
    "doc_id = random.randint(0, len(comments) - 1)\n",
    "\n",
    "# Compare and print the second-most-similar document\n",
    "print('Train Document ({}): «{}»\\n'.format(doc_id, ' '.join(comments[doc_id].words)))\n",
    "sim_id = second_ranks[doc_id]\n",
    "print('Similar Document {}: «{}»\\n'.format(sim_id, ' '.join(comments[sim_id[0]].words)))\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "LAF3aRsHZOp6",
    "liXCdKelZOqE",
    "5p4obNSvZOqG",
    "L6Y4eXciZOqb",
    "p6cjf2BYZOqt"
   ],
   "name": "Week 4-3 Word Embeddings EXTENDED.ipynb",
   "provenance": [
    {
     "file_id": "1aAxdH7FlmzMKph6PXjpxHmM93spH_fkP",
     "timestamp": 1596030806964
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
