{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yPgJH0CIZWyQ"
   },
   "source": [
    "# SISU Digital Humanities: Textual and Language Analysis on Social Media<br />\n",
    "### Session 3: Topic modeling\n",
    "Created by Tom van Nuenen (tom.van_nuenen@kcl.ac.uk) <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m6EAS2bBZWyR"
   },
   "source": [
    "# Topic modeling\n",
    "\n",
    "This notebook introduces topic modeling. Topic modeling is a type of statistical modeling for the discovery of abstract \"topics\" that occur in a collection of documents. It is frequently used in NLP to aid the discovery of hidden semantic structures in a collection of texts.\n",
    "\n",
    "We'll use the `Gensim` package to create our topic models, as it allows us to run tests to optimize our topic amount.\n",
    "\n",
    "After reading this notebook, you'll be able to:\n",
    "\n",
    "1. Use gensim to create topic models;\n",
    "2. Explore the topic models using PyLDAvis;\n",
    "3. Evaluate topic models using several methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 646
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11869,
     "status": "ok",
     "timestamp": 1598020450895,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "EWlJYKM7ZWyT",
    "outputId": "4d03bbb2-bc49-4abe-edfb-e54741f4a4de"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import string\n",
    "import os\n",
    "import pickle\n",
    "from pprint import pprint\n",
    "import re \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "from more_itertools import chunked\n",
    "\n",
    "# NLTK\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Gensim\n",
    "from gensim import corpora, models, similarities\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim\n",
    "\n",
    "# SpaCy \n",
    "import spacy\n",
    "!spacy download en_core_web_sm\n",
    "import en_core_web_sm\n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "# Plotting tools\n",
    "!pip install pyLDAvis\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "pyLDAvis.enable_notebook()\n",
    "\n",
    "import pyLDAvis\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# Logging if you want it\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "# Suppressing it should you want to\n",
    "logging.getLogger().setLevel(logging.CRITICAL)\n",
    "\n",
    "# Suppressing warnings\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\", DeprecationWarning)\n",
    "\n",
    "#SKLearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction import text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3-vcxhnVxhnQ"
   },
   "source": [
    "## Getting the data\n",
    "\n",
    "We'll use the same two datasets as we did in the last notebook: comments and submissions from The Red Pill."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1776,
     "status": "ok",
     "timestamp": 1598020455113,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "8jfT6806Y_PM"
   },
   "outputs": [],
   "source": [
    "trp_sub = pd.read_csv(\"data/TRP-submissions.csv\", lineterminator='\\n')\n",
    "trp_com = pd.read_csv(\"data/TRP-comments.csv\", lineterminator='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I5QWQSm6Cy-M"
   },
   "source": [
    "Get rid of the empty cells in our two DataFrames using the `trp_sub[~trp_sub['selftext'].isin(['[removed]', '[deleted]' ])]` and `.dropna(subset=['selftext'])` (for the submissions DF) and `.dropna(subset=['body'])` (for the comments DF)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 952,
     "status": "ok",
     "timestamp": 1598020460580,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "rp0EerUcZWyc"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "trp_sub = trp_sub[~trp_sub['selftext'].isin(['[removed]', '[deleted]' ])].dropna(subset=['selftext'])\n",
    "trp_com = trp_com[~trp_com['body'].isin(['[removed]', '[deleted]' ])].dropna(subset=['body'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Is2LLBy-ZWyf"
   },
   "source": [
    "## Concatenating submissions and comments\n",
    "\n",
    "For the topic model we're going to make, we will first concatenate the associated submissions and comments (i.e., the threads).\n",
    "\n",
    "Using the `pd.merge()` method, we can do an \"inner join\" of our two DataFrames. This is a relational database operation, which is a common operation in SQL. See [here](https://www.w3schools.com/sql/sql_join.asp) if you want to learn more. An \"inner join\" will yield a new DF which only contains those submissions that have associated comments (based on their \"idstr\" and \"parent\" values).\n",
    "\n",
    "![alt text](https://s33046.pcdn.co/wp-content/uploads/2019/06/venn-diagram-representation-of-sql-inner-join-.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1193,
     "status": "ok",
     "timestamp": 1598020463695,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "iRxPpwrYZWyg"
   },
   "outputs": [],
   "source": [
    "# merge DF based on idstr and parent\n",
    "trp_t = pd.merge(trp_sub, trp_com, how='inner', left_on='idstr', right_on='parent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XFlVeoi8_cfP"
   },
   "source": [
    "We now have a DataFrame containing all original posts / submissions, *and* the comments associated with this original post! Note that this yields a lot of rows with the same \"selftext\" value (as lots of comments refer to the same original post).\n",
    "\n",
    "Also note that this operations has changed the names of the columns: we now have \"x\" and \"y\" columns based on the original DataFrame we took the data from. \"idstr_x\", for instance, refers to the \"idstr\" column from the `trp_sub` DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 893,
     "status": "ok",
     "timestamp": 1598020464062,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "jIy7dsHOZWyo",
    "outputId": "41afe132-89f7-48aa-fbce-6b1bde87af94"
   },
   "outputs": [],
   "source": [
    "trp_t.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dd8xZRYtZWys"
   },
   "source": [
    "Now, let's iterate over our merged DataFrame and group all associated submissions and comments together. \n",
    "- We first create an empty dictionary.\n",
    "- We then iterate over our DataFrame using the Pandas `.iterrows()` method, which allows us to iterate over rows in a DataFrame in a for-loop. It yields a tuple consisting of the index and the row.\n",
    "- `If` the \"idstr_x\" column does not yet exist in the keys of our new dictionary, add it. As the value, add the \"selftext\" (i.e., the original post) and \"body\" (i.e., the first comment) columns.\n",
    "- `Else`, if we have the key already, only add the \"body\" column (i.e., the comment). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 982,
     "status": "ok",
     "timestamp": 1598020469325,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "WtPigWETZWyt"
   },
   "outputs": [],
   "source": [
    "data_d = {}\n",
    "for i, r in trp_t.iterrows():\n",
    "    if r.idstr_x not in data_d.keys():\n",
    "        data_d[r.idstr_x] = [r.selftext, r.body]    \n",
    "    else:\n",
    "        data_d[r.idstr_x].append(r.body)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1094,
     "status": "ok",
     "timestamp": 1598020469720,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "_VDa2iCXZWyw",
    "outputId": "b93a7b6d-c04a-4f96-ef23-bf18b0528e44"
   },
   "outputs": [],
   "source": [
    "# See if it works\n",
    "data_d['t3_3n0dg0']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VPBDsQnhZWy5"
   },
   "source": [
    "Finally, we'll join the items in each of the values in our `dict` and put that in a list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 883,
     "status": "ok",
     "timestamp": 1598020471831,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "ObyRYw--ZWy6"
   },
   "outputs": [],
   "source": [
    "data = ['\\n'.join(thread) for thread in data_d.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-Vy6u-1ZWy9"
   },
   "source": [
    "## Preprocessing\n",
    "\n",
    "First, let's get rid of the newlines. Iterate over each text in our `data` list, and use string replacements or RegEx to remove the newlines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 823,
     "status": "ok",
     "timestamp": 1598020472988,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "X77T7js3ZWy-"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "data = [re.sub(r'\\s+', ' ', txt) for txt in data]\n",
    "\n",
    "data = [txt.replace('\\n', ' ') for txt in data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6HGhm613ZWzB"
   },
   "source": [
    "Time to tokenize. Let's use Gensim's `simple_preprocess()` method this time. If you haven't seen `yield` before, it is used in what's called a generator function. This is simply a function that iterates, instead of only returning something once.\n",
    "\n",
    "`Return` sends a specified value back to its caller whereas `yield` can produce a sequence of values. We should use `yield` when we want to iterate over a sequence, but don’t want to store the entire sequence in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1682,
     "status": "ok",
     "timestamp": 1598020476188,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "4asuvP-EZWzB"
   },
   "outputs": [],
   "source": [
    "def tokenizer(texts):\n",
    "    for text in texts:\n",
    "        yield(gensim.utils.simple_preprocess(text, deacc=True))\n",
    "\n",
    "tokens_list = list(tokenizer(data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eMfGtJvuCDkd"
   },
   "source": [
    "How many threads do we have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1198,
     "status": "ok",
     "timestamp": 1598020492892,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "D-M4m-gjZWzJ",
    "outputId": "e7198300-10c7-4f1e-d0cd-09b36553f9e4"
   },
   "outputs": [],
   "source": [
    "len(tokens_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 979,
     "status": "ok",
     "timestamp": 1598020518034,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "lQAM5k2ccZwO",
    "outputId": "8b654b33-c4de-47f1-a42a-42cf0a2acb8c"
   },
   "outputs": [],
   "source": [
    "tokens_list[1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "q557UWH4CL9k"
   },
   "source": [
    "And how many tokens in total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5434,
     "status": "ok",
     "timestamp": 1595503715024,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "_COVyrIrZWzM",
    "outputId": "d50679b3-ee37-4863-dff7-551aad5e7221"
   },
   "outputs": [],
   "source": [
    "counter = 0\n",
    "for each in tokens_list:\n",
    "    counter += len(each)\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1355,
     "status": "ok",
     "timestamp": 1595504209240,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "FNwkfAV6dd0Z",
    "outputId": "9738582c-3bed-462a-8871-608a1a4b57fa"
   },
   "outputs": [],
   "source": [
    "tokens_list[1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BMY-0LlAZWzP"
   },
   "source": [
    "### Creating N-grams with Gensim\n",
    "\n",
    "Topic modeling – as well as many other kinds of NLP methods – works better when using N-grams, as this allows words that frequently appearing together to be concatenated (e.g. \"red pill\" means something different than \"red\" and \"pill\" separately).\n",
    "\n",
    "Gensim’s `Phrases` model implements bigrams, trigrams, quadgrams, etc. \n",
    "\n",
    "`Phrases` detects phrases based on collocation counts. It builds a model of input text that you then can use on other data.\n",
    "\n",
    "Gensim detects a bigram if a scoring function for two words exceeds a threshold. The two important arguments to `Phrases` are `min_count` and `threshold`. The higher the values of these parameters, the harder it is for words to be combined to bigrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 6330,
     "status": "ok",
     "timestamp": 1598020535504,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "ik3uEPOHZWzP",
    "outputId": "6d43b5b9-febb-4a7e-994a-e2584abc88d6"
   },
   "outputs": [],
   "source": [
    "# Build bigram and trigram models\n",
    "# min_count = min amount of tokens/bigrams in corpus. Threshold = score-based; higher = fewer phrases.\n",
    "bigram = gensim.models.Phrases(tokens_list, min_count=5, threshold=100)\n",
    "trigram = gensim.models.Phrases(bigram[tokens_list], threshold=100)  \n",
    "\n",
    "# `Phraser` must be built from an initial `Phrases` instance. \n",
    "# It then works faster while using much less memory. See https://radimrehurek.com/gensim/models/phrases.html\n",
    "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
    "trigram_mod = gensim.models.phrases.Phraser(trigram)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R9TugyIlZWze"
   },
   "source": [
    "### Stopwords removal, bigrams, lemmatization \n",
    "Let's define some functions for stopword removal, making bigrams and trigrams, and lemmatization (we will use SpaCy for the latter - see [here](https://spacy.io/api/annotation) for more info).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 755,
     "status": "ok",
     "timestamp": 1598020535508,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "4xR2v6tBZWzf"
   },
   "outputs": [],
   "source": [
    "# prepare stopwords\n",
    "stop = set(stopwords.words('english') + ['’', '“', '”', 'nbsp', 'http'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 846,
     "status": "ok",
     "timestamp": 1598020535852,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "cbifDFG3ZWzi"
   },
   "outputs": [],
   "source": [
    "# Define functions for stopwords, bigrams, trigrams and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in doc if word not in stop] for doc in texts]\n",
    "      \n",
    "def make_bigrams(texts):\n",
    "    return [bigram_mod[doc] for doc in texts]\n",
    "\n",
    "def make_trigrams(texts):\n",
    "    return [trigram_mod[bigram_mod[doc]] for doc in texts]\n",
    "\n",
    "def lemmatization(texts, allowed_postags=['NOUN']):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for doc in texts:\n",
    "        joined = nlp(\" \".join(doc)) \n",
    "        texts_out.append([token.lemma_ for token in joined if token.pos_ in allowed_postags])\n",
    "    return texts_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11485,
     "status": "ok",
     "timestamp": 1598020547873,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "NHm2CiAiZWzl"
   },
   "outputs": [],
   "source": [
    "# Remove stopwords\n",
    "tokens_no_stops = remove_stopwords(tokens_list)\n",
    "\n",
    "# Form trigrams\n",
    "trigrams = make_trigrams(tokens_no_stops)\n",
    "\n",
    "# Do lemmatization \n",
    "lemmas = lemmatization(trigrams)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 9483,
     "status": "ok",
     "timestamp": 1598020547874,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "4Z4sSSibe4Cj",
    "outputId": "e8a3d291-f8f0-47df-ddee-4333dde57279"
   },
   "outputs": [],
   "source": [
    "lemmas[1][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rVgem81TZWzu"
   },
   "source": [
    "### Pickling\n",
    "We've done a lot of work, so let's save this object in a pickle. Any object in Python can be pickled so that it can be saved on disk (or in this case, a virtual disk). What pickle does is that it “serializes” the object first before writing it to file. Pickling is a way to convert a python object (list, dict, etc.) into a character stream. The idea is that this character stream contains all the information necessary to reconstruct the object in another python script – or to fetch it at a later time when you don't want to do the computing work all over again.\n",
    "\n",
    "Note that when pickling, you can use any file extension name you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ruq5zrEvZWzv"
   },
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "with open(\"TRP_nouns.text\", \"wb\") as docP: \n",
    "    pickle.dump(lemmas, docP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 4393,
     "status": "ok",
     "timestamp": 1595504543384,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "nFYnd6p9eyoY",
    "outputId": "bee46b5a-c71b-459b-bb32-5d22b4a0bfbc"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "04ELm1XGeyLc"
   },
   "outputs": [],
   "source": [
    "# should you want to load in the pickle we just saved:\n",
    "with open(\"TRP_nouns.text\", \"rb\") as docP: \n",
    "    lemmas = pickle.load(docP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BNmgAQEoZWzx"
   },
   "source": [
    "## Creating a `Dictionary` with Gensim\n",
    "\n",
    "Now, let's create our gensim dictionary - a mapping of each word to a unique id. \n",
    "It will be used to create a `Corpus` object, which is gensim’s equivalent of a Document-Term matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1167,
     "status": "ok",
     "timestamp": 1598020624259,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "v5U9p6rrZWzy"
   },
   "outputs": [],
   "source": [
    "# Create Dictionary \n",
    "dictionary = corpora.Dictionary(lemmas)\n",
    "\n",
    "# Create Corpus, i.e. Document-Term Matrix\n",
    "corpus = [dictionary.doc2bow(text) for text in lemmas]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 671,
     "status": "ok",
     "timestamp": 1598020755447,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "1137g7AVdZDG",
    "outputId": "47cfd087-eb4e-4e78-e80c-3effad3da9cb"
   },
   "outputs": [],
   "source": [
    "dictionary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vjopps2qZWz0"
   },
   "source": [
    "*Note that Gensim allows you to run the `.add_documents` method that will append additional documents to your `dictionary`, if you need to.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mYZAQn44ZWz1"
   },
   "source": [
    "Let's view some of the corpus we have now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 935,
     "status": "ok",
     "timestamp": 1598020669113,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "9h7h3ms4ZWz1",
    "outputId": "74d2d4da-bdec-422d-c459-9643f3cbaab3"
   },
   "outputs": [],
   "source": [
    "print(corpus[0][:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eXRXTfMgZW0A"
   },
   "source": [
    "Observe the first 10 tuples above. Each consists of words with a unique id. This a mapping of (word_id, word_frequency). For example, (0, 1) above demonstrates that word id 0 occurs once in the first document. Word id 5 occurs 4 times, and so on. This is used as the input by the LDA model.\n",
    "\n",
    "If you want to see what word a given id corresponds to, pass the id as a key to the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 836,
     "status": "ok",
     "timestamp": 1598020696293,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "0pKC0MBiZW0A",
    "outputId": "b055953b-2856-4584-fb9b-7da13a0020cd"
   },
   "outputs": [],
   "source": [
    "dictionary[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QNr34ffPZW0D"
   },
   "source": [
    "And if you want to see the associated id for some word:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 938,
     "status": "ok",
     "timestamp": 1598020699425,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "dNhZ-fZ4ZW0D",
    "outputId": "90c87970-e5ea-459e-d829-8791af6f04f2"
   },
   "outputs": [],
   "source": [
    "dictionary.token2id['advance']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uep-_3ImZW0S"
   },
   "source": [
    "## Running an LDA model\n",
    "\n",
    "Now, let's run a Gensim LDA model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 8663,
     "status": "ok",
     "timestamp": 1598020709803,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "5ZPAUhOIZW0T"
   },
   "outputs": [],
   "source": [
    "## Build LDA model. Make sure to play around with chunksize and passes and check if coherence score changes a lot.\n",
    "\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                           id2word=dictionary,\n",
    "                                           num_topics=20, \n",
    "                                           random_state=100,\n",
    "                                           # eval_every = 20, # this is evaluation, perplexity\n",
    "                                           update_every=1,\n",
    "                                           chunksize=500,\n",
    "                                           passes=10,\n",
    "                                           alpha='auto',\n",
    "                                           per_word_topics=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JWRO12UXZW0d"
   },
   "source": [
    "## Visualizing the model\n",
    "\n",
    "Let's try to evaluate our topics. First, we can visualize our topics using pyLDAvis. A \"good\" topic model produces non-overlapping, fairly large bubbles, which should be scattered throughout the chart instead of being clustered in one quadrant. A model with too many topics will typically have many overlaps, small sized bubbles clustered in one region of the chart. **This is the first way in which you can evaluate your topic models**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 861
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 10193,
     "status": "ok",
     "timestamp": 1598020723457,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "Aze6E-0ZZW0e",
    "outputId": "139d1f7a-db03-4281-bf1c-cb0c253c4d0e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# feed the LDA model into the pyLDAvis instance\n",
    "lda_viz = gensimvis.prepare(lda_model, corpus, dictionary)\n",
    "lda_viz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jOGnQ-WiZW0h"
   },
   "source": [
    "### PyLDAvis graph\n",
    "\n",
    "On the left, there is a 2D plot of the \"distance\" between all of the topics (labeled as the Intertopic Distance Map). This plot uses a multidimensional scaling (MDS) algorithm. \n",
    "- Similar topics should appear close together on the plot; dissimilar topics should appear far apart. \n",
    "- The relative size of a topic's circle in the plot corresponds to the relative frequency of the topic in the corpus.\n",
    "\n",
    "### Exploring topics and words\n",
    "- You can scrutinize a topic more closely by clicking on its circle, or entering its number in the \"selected topic\" box in the upper-left (Note that, though the data used by gensim and pyLDAvis are the same, they don't use the same ID numbers for topics.)\n",
    "- If you roll your mouse over a term in the bar chart on the right, the topic circles will resize in the plot on the left. This shows the strength of the relationship between the topics and the selected term.\n",
    "\n",
    "### Salience\n",
    "On the right, there is a bar chart with the top terms. When no topic is selected in the plot on the left, the bar chart shows the top-30 most **salient** terms in the corpus. A term's saliency is a measure of both how frequent the term is in the corpus and how \"distinctive\" it is in distinguishing between different topics.\n",
    "\n",
    "### Probability Vs Exclusivity \n",
    "When you select a particular topic, this bar chart changes to show the top-30 most \"relevant\" terms for the selected topic. The relevance metric is controlled by the parameter λ, which can be adjusted with a slider above the bar chart:\n",
    "\n",
    "* Setting λ close to 1.0 (the default) will rank the terms according to their probability within the topic.\n",
    "* Setting λ close to 0.0 will rank the terms according to their \"distinctiveness\" or \"exclusivity\" within the topic. This means that terms that occur only in this topic, and do not occur in other topics.\n",
    "\n",
    "You can move the slider between 0.0 and 1.0 to weigh term probability and exclusivity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPmoNRMoZW0h"
   },
   "source": [
    "## Assignment: Analyzing the pyLDA visualization\n",
    "\n",
    "The interactive visualization pyLDAvis produces is helpful for both **individual** topics: you can manually select each topic to view its top most frequent and/or \"relevant\" terms, using different values of the λ parameter. This can help when you're trying to assign a name or \"meaning\" to each topic. \n",
    "\n",
    "It also helps you to see the **relationships** between topics: exploring the Intertopic Distance Plot can help you learn about how topics relate to each other, including potential higher-level structure between groups of topics.\n",
    "\n",
    "See if you can make sense of the patterns you are seeing. Can you give an explanation for the distance between certain topics?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 915,
     "status": "ok",
     "timestamp": 1595517235869,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "Es44L0TZNC8R",
    "outputId": "060a09e4-8ea3-4e57-cde6-f1182d8206bc"
   },
   "outputs": [],
   "source": [
    "# ADDING A DOC\n",
    "test_doc = ['alpha men are better than beta men.', 'indeed, alpha men are much cooler.']\n",
    "bow_test_doc = dictionary.doc2bow(test_doc)\n",
    "lda_model.get_document_topics(bow_test_doc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 357
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 843,
     "status": "ok",
     "timestamp": 1595517208836,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "Bv_FWdFSPHN0",
    "outputId": "a0c1112e-4457-4603-c229-a58059322ee9"
   },
   "outputs": [],
   "source": [
    "topics = lda_model.print_topics(num_words=5)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "osn95IYrZW0i"
   },
   "source": [
    "## Calculating Topic Coherence and Perplexity\n",
    "\n",
    "Next, we can apply some statistical measures to help us determine the optimal number of topics in our topic model.\n",
    "\n",
    "**Topic Coherence** is a measure applied to the top N words from each topic. It is defined as the average / median of the pairwise word-similarity scores of the words in the topic. This helps to distinguish between topics that are semantically interpretable topics, and topics that are artifacts of statistical inference. \n",
    "\n",
    "A set of statements or facts is said to be coherent if the statements support each other. An example of a coherent fact set is “the game is a team sport”, “the game is played with a ball”, “the game demands great physical efforts”\n",
    "\n",
    "A good model will generate topics with *high* topic coherence scores. Good topics are topics that can be described by a short label, therefore this is what the topic coherence measure should capture.\n",
    "\n",
    "There are different ways to calculate semantic similarity, e.g. through normalized pointwise mutual information (NPMI) or through cosine similarity.\n",
    "\n",
    "**Perplexity** is a measure of how well a probability model predicts a sample.\n",
    "It captures how surprised a model is of new data it has not seen before, and is measured as the normalized log-likelihood of a held-out test set. The model with the *lowest* perplexity is generally considered the “best”. \n",
    "\n",
    "*The issue with perplexity is that it tends to not be strongly correlated to human judgment and, even sometimes slightly anti-correlated. Therefore, we'll work with coherence scores*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 25959,
     "status": "ok",
     "timestamp": 1595505061478,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "wQFU0eNUZW0i",
    "outputId": "df59a55f-040d-42f8-e321-748ca9d227f1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_model.log_perplexity(corpus))  # A measure of how good the model is. The lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model = CoherenceModel(model=lda_model, corpus=corpus, texts=lemmas, dictionary=dictionary, coherence='c_v') \n",
    "# The higher the better. A coherence score of .4 means probably not right number of topics; .6 is great. Anything more is suspiciously great.\n",
    "coherence = coherence_model.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xg-5r0htZW0k"
   },
   "source": [
    "There's no hard or fast rule on what makes a good coherence or perplexity score. We have to compare this for different iterations of our topic model (using different amounts of topics) to see which one works best."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RjFdiMk6ZW0v"
   },
   "source": [
    "## Optimizing coherence scores\n",
    "\n",
    "The most obvious thing we can do to find optimal scores is to play around with the amount of topics our model creates. One way to do this is to build many LDA models with different values of number of topics (k), and then pick the one that gives the highest coherence value. Choosing a ‘k’ at the end of a rapid growth of topic coherence usually yields meaningful and interpretable topics. If you see the same keywords being repeated in multiple topics, it’s probably a sign that the ‘k’ is too large.\n",
    "\n",
    "This `compute_coherence_values()` function trains multiple LDA models, provides the models, and tells you their corresponding coherence scores.\n",
    "\n",
    "Also note the docstring I create here: these are documentation for the functions we create. It describes what a function does, and can be called using `help(function_X)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oOZRa_4LZW0w"
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    total_amount = limit / step\n",
    "    current_amount = 0\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamodel.LdaModel(corpus=corpus, id2word=dictionary, num_topics=num_topics, random_state=100, update_every=1, \n",
    "                                chunksize=500, passes=10, alpha='auto', per_word_topics=False)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        # When using 'c_v' texts should be provided, corpus isn’t needed. \n",
    "        # When using ‘u_mass’ corpus should be provided, if texts is provided, it will be converted to corpus using the dictionary \n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "        current_amount += 1\n",
    "        print(\"Built \" + str(current_amount) + \" of \" + str(total_amount) + \" models\")\n",
    "\n",
    "    return model_list, coherence_values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 306
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1169,
     "status": "ok",
     "timestamp": 1595505679519,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "NTH4i85HeD3j",
    "outputId": "4e9dc82b-313a-4d5c-9e21-ccc1137eb1db"
   },
   "outputs": [],
   "source": [
    "help(compute_coherence_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eX4Mc6JZo4sG"
   },
   "source": [
    "Using our new function, let's run a bunch of topic models with different amounts of topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1397306,
     "status": "ok",
     "timestamp": 1595507080302,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "LYWcr2NhZW0z",
    "outputId": "27bdccee-ae4f-401e-88a0-da22f6342e14"
   },
   "outputs": [],
   "source": [
    "# Can take a long time to run.\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=lemmas, start=10, limit=100, step=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ks6jTrgtpC1m"
   },
   "source": [
    "Now, from all those models, let's visualize the output of the coherence scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1396151,
     "status": "ok",
     "timestamp": 1595507080304,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "td7Zi7YpZW03",
    "outputId": "839b5408-93d9-4210-c71b-87aa17396ca1",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Show graph\n",
    "limit=100; start=10; step=10;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 170
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1394838,
     "status": "ok",
     "timestamp": 1595507080305,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "2zX73ICjZW05",
    "outputId": "a4823bb4-9a6f-4a73-d36f-68ffdd43d11b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Print these coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\" Num Topics =\", m, \"Coherence Value =\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DPwvgE04ZW0_"
   },
   "source": [
    "If the coherence score seems to keep increasing, it generally makes sense to pick the model that gave the highest CV before dropping again. We'll pick the most fitting amount of topics to continue our research."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MjkQGZ9W6ikZ"
   },
   "outputs": [],
   "source": [
    "# Save to disk\n",
    "with open(\"optimal_model.model\", \"wb\") as docP: \n",
    "    pickle.dump(model_list[4], docP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 3622,
     "status": "ok",
     "timestamp": 1595511877186,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "ntvQrCcc6ikf",
    "outputId": "e7dbde89-4f85-4c19-f88c-ccfe84e5c35b"
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Z-e-p-xN6iki"
   },
   "outputs": [],
   "source": [
    "# should you want to load in the pickle we just saved:\n",
    "with open(\"optimal_model.model\", \"rb\") as docP: \n",
    "    optimal_model = pickle.load(docP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1607,
     "status": "ok",
     "timestamp": 1595511788577,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "h3IDZcmfZW1A",
    "outputId": "18afe590-c8a0-40ff-fc83-47c1f598eb2f"
   },
   "outputs": [],
   "source": [
    "# Select the ideal model and print the topics\n",
    "optimal_model = model_list[4]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9fK6uo1EZW1M"
   },
   "source": [
    "## Finding most dominant topic per thread\n",
    "\n",
    "One of the practical application of topic modeling is to determine what topic a given Reddit thread is about. To figure this out, we find the topic number that has the highest percentage contribution in that thread. We'll write a `dominant_topic()` function that aggregates this information in a presentable table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1596,
     "status": "ok",
     "timestamp": 1595508219005,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "oHB0mWZAszAa",
    "outputId": "a3418be0-0729-4668-a74c-27f357b54896"
   },
   "outputs": [],
   "source": [
    "optimal_model[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 42600,
     "status": "ok",
     "timestamp": 1595508803138,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "iz_TBN_dZW1Q",
    "outputId": "6d38bbf6-f023-4271-ec05-e5d361e37c07",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def dominant_topic(ldamodel=optimal_model, corpus=corpus, texts=lemmas):\n",
    "    # Create DF\n",
    "    thread_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each thread\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                thread_topics_df = thread_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    thread_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    thread_topics_df = pd.concat([thread_topics_df, contents], axis=1)\n",
    "    return thread_topics_df \n",
    "\n",
    "df_topic_thread_keywords = dominant_topic(ldamodel=optimal_model, corpus=corpus, texts=data)\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_thread_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v_OLtA9Wtq2F"
   },
   "source": [
    "Retrieving dominant topics via `.loc`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bQBSqG5QtSud"
   },
   "outputs": [],
   "source": [
    "df_dominant_topic.loc[df_dominant_topic['Dominant_Topic'] == 4.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7m1D27LrZW1S"
   },
   "source": [
    "## Finding most distinctive threads per topic\n",
    "\n",
    "We can also find the threads that include the highest amount of words for a certain topic. You could use this if you have found a really interesting topic, and you want to know the top threads this topic is typically found in. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1212,
     "status": "ok",
     "timestamp": 1595508830818,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "fh8mI2x-ZW1T",
    "outputId": "15a90d58-9a31-46a3-bfee-970d3500c952",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Group top 5 threads under each topic\n",
    "thread_topics_sorteddf = pd.DataFrame()\n",
    "\n",
    "thread_topics_outdf_grpd = df_topic_thread_keywords.groupby('Dominant_Topic')\n",
    "\n",
    "for i, grp in thread_topics_outdf_grpd:\n",
    "    thread_topics_sorteddf = pd.concat([thread_topics_sorteddf, \n",
    "                                             grp.sort_values(['Perc_Contribution'], ascending=[0]).head(5)], \n",
    "                                            axis=0)\n",
    "\n",
    "# Reset Index    \n",
    "thread_topics_sorteddf.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Format\n",
    "thread_topics_sorteddf.columns = ['Topic_Num', \"Topic_Perc_Contrib\", \"Keywords\", \"Text\"]\n",
    "\n",
    "# Show\n",
    "thread_topics_sorteddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1409,
     "status": "ok",
     "timestamp": 1595508855516,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "7zNsKAM2ZW1W",
    "outputId": "359f083b-1c42-4235-ccec-4513a53a362d"
   },
   "outputs": [],
   "source": [
    "thread_topics_sorteddf['Text'][4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "H-lVwjhpZW1a"
   },
   "source": [
    "## Topic distribution across threads\n",
    "\n",
    "Finally, we can look at the volume and distribution of topics in order to judge how widely each topic is discussed in our threads. The below table exposes that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 752,
     "status": "ok",
     "timestamp": 1595508913566,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "thVTIHsMZW1b",
    "outputId": "3ff55ef7-1521-45f8-fbd6-549fa2429eca"
   },
   "outputs": [],
   "source": [
    "# Number of Threads for Each Topic\n",
    "topic_counts = df_topic_thread_keywords['Dominant_Topic'].value_counts()\n",
    "\n",
    "# Percentage of Threads for Each Topic\n",
    "topic_contribution = round(topic_counts/topic_counts.sum(), 4)\n",
    "\n",
    "# Topic Number and Keywords\n",
    "topic_num_keywords = df_topic_thread_keywords[['Dominant_Topic', 'Topic_Keywords']]\n",
    "\n",
    "# Concatenate Column wise\n",
    "df_dominant_topics = pd.concat([topic_num_keywords, topic_counts, topic_contribution], axis=1)\n",
    "\n",
    "# Change Column names\n",
    "df_dominant_topics.columns = ['Dominant_Topic', 'Topic_Keywords', 'Num_Documents', 'Perc_Documents']\n",
    "\n",
    "# Show\n",
    "df_dominant_topics[:10]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jOGnQ-WiZW0h"
   ],
   "name": "Week 3-4 Evaluating topic models ANSWERS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
