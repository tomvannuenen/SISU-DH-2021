{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bhs6S4deZFWW"
   },
   "source": [
    "# SISU Digital Humanities: Textual and Language Analysis on Social Media<br />\n",
    "### Session 2: Preprocessing and tf-idf\n",
    "Created by Tom van Nuenen (tom.van_nuenen@kcl.ac.uk) <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8ekf1a33ZFWY"
   },
   "source": [
    "# Preprocessing and comparing subreddits\n",
    "\n",
    "Today we will (1) learn how to preprocess text in a DataFrame, and (2) learn about tf-idf. Tf-idf allows us to compare different related subreddits, in order to find the most distinctive words in a particular subreddit. It can also help us to find similar posts to ones we're interested in.\n",
    "\n",
    "**After completing this notebook, you will be able to:**\n",
    "- Preprocess social media data, including removing punctuation, tokenizing, and lemmatizing;\n",
    "- Understand how tf-idf can be used to compare datasets;\n",
    "- Find most-distinctive words in a subreddit using tf-idf;\n",
    "- Find similar posts using tf-idf."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pXhTdS5Ph2ND"
   },
   "source": [
    "## Import packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "executionInfo": {
     "elapsed": 1765,
     "status": "ok",
     "timestamp": 1594912396864,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "-uQdxtmCZFWY",
    "outputId": "0099f206-431f-4e7a-c76a-aafaf64063d8"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "import re \n",
    "import string\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7RfwrCNiGvn"
   },
   "source": [
    "Let's get our files. These three datasets are taken from an American social media platform that is organized around interest communities (like Hupu or Douban in China). \n",
    "\n",
    "The three communities we have here are, to different degrees, related to the \"Manosphere\" (see https://en.wikipedia.org/wiki/Manosphere for more). We might say they are in the same \"genre\" of discourse, which arguably allows us to compare them.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Awlk7rs7ZFW4"
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "trp = pd.read_csv(\"data/TRP-submissions.csv\", lineterminator=\"\\n\")\n",
    "sed = pd.read_csv(\"data/seduction.csv\", lineterminator=\"\\n\")\n",
    "mgtow = pd.read_csv(\"data/mgtow.csv\", lineterminator=\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5LusX7JXckS"
   },
   "source": [
    "How big are our datasets?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "executionInfo": {
     "elapsed": 16475,
     "status": "ok",
     "timestamp": 1594912411980,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "J433TZm4XcEa",
    "outputId": "f8290c4c-a7af-4d5a-8dfe-3f67f502d0df"
   },
   "outputs": [],
   "source": [
    "print(\"seduction: \" + str(len(sed)))\n",
    "print(\"redpill: \" + str(len(trp)))\n",
    "print(\"men going their own way: \" + str(len(mgtow)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dVKTaN8LutK"
   },
   "source": [
    "### Removing rows\n",
    "Missing values (`NaN`) in a DataFrame can cause a lot of errors. In general, it's a god idea to get rid of those rows whose \"selftext\" is missing. Here's an example of how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 235
    },
    "executionInfo": {
     "elapsed": 16432,
     "status": "ok",
     "timestamp": 1594912411981,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "6khNIe0kLvOf",
    "outputId": "f7b5dc7b-b7bb-4a40-813f-0e7c9224d9a0"
   },
   "outputs": [],
   "source": [
    "data = {'Name':['Sai', 'Jack', 'Angela', 'Matt', 'Alisha', 'Ricky'],'Age':[28,34,None,42, \"[removed]\", \"[deleted]\"]}\n",
    "df = pd.DataFrame(data) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 16395,
     "status": "ok",
     "timestamp": 1594912411982,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "n0gj5FBXM8NG",
    "outputId": "ade9970f-248b-4896-c0d9-050762679ae6"
   },
   "outputs": [],
   "source": [
    "clean_df = df.dropna(subset=['Age'])  # Drop NaN in the column 'Age'\n",
    "clean_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTWDsJFgYRle"
   },
   "source": [
    "We can also remove cells with particular text in it (this is relevant as Reddit datasets often contain posts that are removed or deleted!). We do this by using the `.isin()` method. We remove this selection from our DataFrame by using Python's bitwise NOT operator, `~`. See if you understand how this works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 142
    },
    "executionInfo": {
     "elapsed": 16353,
     "status": "ok",
     "timestamp": 1594912411983,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "IMqUgnNdYBHW",
    "outputId": "720578a6-7172-4892-a2bd-76b1fd1fd7e6"
   },
   "outputs": [],
   "source": [
    "cleaner_df = clean_df[~clean_df['Age'].isin(['[removed]', '[deleted]' ])]\n",
    "cleaner_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EaNTzUNcNqmz"
   },
   "source": [
    "Your turn! Drop the missing (`NaN`), removed (`[removed]`) and deleted (`[deleted]`) values from our three DataFrames and assign the result to the same variable names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Othbm6juMVmC"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HK8aZS3SZyZB"
   },
   "source": [
    "Let's see if that shrinks our DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "executionInfo": {
     "elapsed": 995,
     "status": "ok",
     "timestamp": 1594912431168,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "duFmEc9OZx00",
    "outputId": "a0deab90-2ae8-4705-e4e2-54cd0e808a47"
   },
   "outputs": [],
   "source": [
    "print(\"seduction: \" + str(len(sed)))\n",
    "print(\"theredpill: \" + str(len(trp)))\n",
    "print(\"mgtow: \" + str(len(mgtow)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s1wWfN7XZ1oE"
   },
   "source": [
    "Looks like it did!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xlu74LCjjNhm"
   },
   "source": [
    "### Getting a slice\n",
    "It's usually good to start small and see if all of your preprocessing functions work as expected, then scale up. Let's start with a slice of 100 posts based on the highest score. Yesterday, we saw how we can do that using the `.sort_values()` method. Recall that sorting and slicing a dataframe works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcpxg8ezjNxA"
   },
   "outputs": [],
   "source": [
    "# Sorting\n",
    "sorted_df = trp.sort_values(by=['score'], ascending=False)\n",
    "\n",
    "# Slicing\n",
    "sliced_df = trp[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ON6sYHXbiDac"
   },
   "source": [
    "Let's combine these two expressions to filter the 10 highest-scoring posts of `trp`. We assign it to a new variable: `trp_10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elYKcpP0ZFXB"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pn6giFWubji1"
   },
   "source": [
    "## Preprocessing data\n",
    "\n",
    "Great, we got our data. Now, we need to preprocess it. This includes:\n",
    "1. Removing special characters and punctuation\n",
    "2. Tokenizing\n",
    "3. Removing stopwords\n",
    "4. Part of Speech (POS) tagging & filtering\n",
    "5. Stemming / lemmatizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rEaV-cyYbjjm"
   },
   "source": [
    "### Removing punctuation\n",
    "\n",
    "First, have a look at how to use `string.punctuation` to get rid of some punctuation characters. `string.punctuation` is not a function: it's a pre-initialized string which we can use to get rid of punctuation in a string.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 2879,
     "status": "ok",
     "timestamp": 1594912447754,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "EW9WvfCTmMQ2",
    "outputId": "f5b0e565-eef8-44c0-a29f-831aeb552e1f"
   },
   "outputs": [],
   "source": [
    "old_sent = \"I. don't. know. why. I'm. speaking. like. this.\"\n",
    "new_sent = \"\"\n",
    "for ch in old_sent:\n",
    "  if ch not in string.punctuation:\n",
    "    new_sent += ch\n",
    "\n",
    "new_sent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cXwW6hxsmxYk"
   },
   "source": [
    "Your turn! Try to create a function called `strip_punctuation` that strips punctuation from a string. It takes a string as a parameter, and returns a new string with all punctuation stripped out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j3NPw95Objjn"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8Bme_0wjuuF"
   },
   "source": [
    "Try to see if it works:\n",
    "\n",
    "1. Create an empty list called `trp_strip_punct`;\n",
    "2. Run a `for`-loop that iterates over all the \"selftexts\" in the `trp_10` DataFrame, and that applies your function to each; \n",
    "3. Save the result in a new variable;\n",
    "4. Print your new variable to see if it worked!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 207
    },
    "executionInfo": {
     "elapsed": 3515,
     "status": "ok",
     "timestamp": 1594912451192,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "VGVq8rYRjwZp",
    "outputId": "1d59974e-5d83-4f35-b14c-0a497f817f74",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good, except it looks like our function removed the punctuation between URLs, as well as some escaped newlines (`\\n`) that are left over. We will learn how to deal with those later on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sAGyS-eEqQTk"
   },
   "source": [
    "### Tokenizing\n",
    "Next, we need to create a tokenizer. Create another list called `trp_tokens`, then use another for-loop that applies NLTK's `word_tokenize()` method on each entry of our `trp_strip_punct` list. Use `.append()` instead of `.extend()` so that your loop creates 100 lists of tokens (instead of one long list of tokens, like we did yesterday). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXwV2ofxbji2"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcutO8_8ltVp"
   },
   "source": [
    "`trp_tokens` is a list of lists: each list contains the individual tokens of a post. What if we want to access a list within a list? It works like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 778,
     "status": "ok",
     "timestamp": 1594912464473,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "nfkKqBPels49",
    "outputId": "bda6d732-78eb-4a49-9242-dd72c9ed7644"
   },
   "outputs": [],
   "source": [
    "list1 = [[10,13,17],[3,5,1],[13,11,12]]\n",
    "list1[0][2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5NYhlQlalJih"
   },
   "source": [
    "Your turn! Print out the first 10 entries in the first entry of the `trp_tokens` list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "executionInfo": {
     "elapsed": 1599,
     "status": "ok",
     "timestamp": 1594912466165,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "W2FDyZXNbji7",
    "outputId": "6e456dcf-15ca-4bad-da3b-b8a34ee9c20d",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6rNmrlsxsGbB"
   },
   "source": [
    "### Programming basics: Sets\n",
    "Do these exercises if you need to learn about sets!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToZw1ALtnnXc"
   },
   "source": [
    "A set is an **unordered** and **unindexed** collection. This makes them different from lists, which are ordered, and from dictionaries, which are indexed. You can use sets to rapidly iterate through a list, when the order within that list doesn't matter. \n",
    "\n",
    "In Python sets are written with curly brackets, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1333,
     "status": "ok",
     "timestamp": 1594912472150,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "3LnqWzBescvZ",
    "outputId": "09ced806-d716-4fb5-aaa1-6cee4af6796b"
   },
   "outputs": [],
   "source": [
    "my_set = {\"apple\", \"pear\", \"orange\"}\n",
    "print(my_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXt8whadslGm"
   },
   "source": [
    "Note that the order is not preserved!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfOI3Zm2bjj1"
   },
   "source": [
    "### Removing stopwords\n",
    "Next, let's remove stopwords. We can do so using NLTK's stopwords list, which we imported above. Let's have a look at some of these stopwords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1589,
     "status": "ok",
     "timestamp": 1594912474395,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "15WB6xkKqquL",
    "outputId": "6e5bca26-040d-488a-b7b7-13efe6ab5d12"
   },
   "outputs": [],
   "source": [
    "stopwords.words('english')[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YBP6qhcrAvf"
   },
   "source": [
    "Iterating through this list for *every* word in our two corpora is going to take a long time, so let's turn it into a set. This saves us some time, as sets are less memory-intensive.\n",
    "\n",
    "Remember, when creating a set it shouldn't matter which order items are in – and for our stopwords list, that is the case!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1083,
     "status": "ok",
     "timestamp": 1594912474396,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "JXYe77ZjtDtX",
    "outputId": "1ba7d2e8-cb92-441e-ff4e-59d5d1b6b4e2"
   },
   "outputs": [],
   "source": [
    "set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PkkZ6pV3_e6Y"
   },
   "source": [
    "Your turn! \n",
    "1. Create a function called `strip_stopwords()` that takes `tokens` as a parameter;\n",
    "2. In the function, create a list named `no_stop`; \n",
    "3. Turn `stopwords.words('english')` into a set (like above), then assign it to a variable named `stop`;\n",
    "4. Run a for-loop that fills the `no_stop` list with only those tokens that are *not* in `stop` (you need an `if`-statement!);\n",
    "5. Finally, `return` the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MO_6RsDkbjj1"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FCf2ACawAVTe"
   },
   "source": [
    "Run the following line of code to see if it worked. You should get a printout of the first 10 tokens in the first post of `trp` – without the stopwords of course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "executionInfo": {
     "elapsed": 1737,
     "status": "ok",
     "timestamp": 1594912475812,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "Vxa_E-XEbjj-",
    "outputId": "c0474080-8c21-4489-f62b-96ba8b058e6f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Run this\n",
    "trp_clean = [strip_stopwords(tokens) for tokens in trp_tokens]\n",
    "trp_clean[0][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S3czfumObjjF"
   },
   "source": [
    "### Stemming\n",
    "Tokenizers are great, but they're often not perfect. Look at the example below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 1318,
     "status": "ok",
     "timestamp": 1594912477595,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "EmNSZkGLbjjH",
    "outputId": "7a910429-23d1-45e7-b5ac-df1f621e2ae7",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "word_tokenize(\"Why won't this work?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ml1MyMB-bjjM"
   },
   "source": [
    "Looks like it did a pretty good job, except it considers \"wo\" and \"n't\" as different words.. Annoying. This is where **stemming** and **lemmatizing** come in handy. These are two text normalization techniques that are used to prepare text, words, and documents for further processing. \n",
    "\n",
    "See [this link](https://www.datacamp.com/community/tutorials/stemming-lemmatization-python?utm_source=adwords_ppc&utm_campaignid=1455363063&utm_adgroupid=65083631748&utm_device=c&utm_keyword=&utm_matchtype=b&utm_network=g&utm_adpostion=&utm_creative=332602034361&utm_targetid=dsa-429603003980&utm_loc_interest_ms=&utm_loc_physical_ms=1012831&gclid=Cj0KCQjwgJv4BRCrARIsAB17JI4kMKOUrJcdearlvPx4kl3VNVcqeZz-oeTSlbgikK3tJbXMrAmWTCwaAvUzEALw_wcB) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A9ZVhgbdbjjN"
   },
   "source": [
    "**Stemming** is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language. First, let's load our stemmer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AxFw-sobjjN"
   },
   "outputs": [],
   "source": [
    "stemmer = nltk.stem.LancasterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "executionInfo": {
     "elapsed": 1859,
     "status": "ok",
     "timestamp": 1594912480344,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "_Xt4sEh0bjjR",
    "outputId": "cba4d878-5573-4864-f5c9-7c3078bf1f2e",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for each in [\"think\", \"thinker\", \"thinking\"]:\n",
    "    print(stemmer.stem(each))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ltayyO3SbjjU"
   },
   "source": [
    "...but stemming doesn't always produce the prettiest results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "executionInfo": {
     "elapsed": 1906,
     "status": "ok",
     "timestamp": 1594912481145,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "5muPaUpubjjV",
    "outputId": "ccdf128f-1b96-4402-a7f3-d431b7793dfb"
   },
   "outputs": [],
   "source": [
    "for each in [\"create\", \"creating\", \"creator\"]:\n",
    "    print(stemmer.stem(each))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJ5EYLZnbjja"
   },
   "source": [
    "### Lemmatizing\n",
    "A lemma is the canonical, dictionary or citation form of a word. For instance, the lemma for \"thinks\" is \"think.\" Lemmatization, in other words, is the process of converting a word to its base form.\n",
    "\n",
    "Lemmatizing your data typically is a bit less intrusive than stemming it. Let's see it in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7DHGwVwDbjjb"
   },
   "outputs": [],
   "source": [
    "lemmatizer = nltk.stem.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "executionInfo": {
     "elapsed": 4616,
     "status": "ok",
     "timestamp": 1594912484746,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "Fx4Zwmjgbjjf",
    "outputId": "60e781e6-f072-4ba9-fbd5-bf565e168dff"
   },
   "outputs": [],
   "source": [
    "for each in [\"trade\", \"trades\", \"trading\", \"trader\", \"traders\"]:\n",
    "    print(lemmatizer.lemmatize(each))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AyQ4CdYrwfg8"
   },
   "source": [
    "Your turn! \n",
    "1. Create a function called `lemmatize()` that takes `tokens` as a parameter;\n",
    "2. Create a new list called `lemmas`\n",
    "3. In the function, assign `nltk.stem.WordNetLemmatizer()` to a variable called `lemmatizer`, like above; \n",
    "4. Run a `for`-loop that uses `lemmatizer.lemmatize(each)` to lemmatize each token in `tokens`; append the output to our `lemmas` list;  \n",
    "5. Finally, `return` the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3Zk1vuxMwfg9"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XW3YzyYGx_3v"
   },
   "source": [
    "Run the following line of code to see if it worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trp_clean[0][20:30]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 3675,
     "status": "ok",
     "timestamp": 1594912484748,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "SZ32Gkk7x_3v",
    "outputId": "5f68b0f0-c7cf-421b-80c0-b1ea308c9962"
   },
   "outputs": [],
   "source": [
    "# Run this\n",
    "trp_lemmas = [lemmatize(tokens) for tokens in trp_clean]\n",
    "trp_lemmas[0][20:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0wqgYPvK2rL7"
   },
   "source": [
    "### Forcing to string\n",
    "Sometimes, when we have a list, we actually want a string. For instance, some libraries of NLP tools require strings as input. In those cases, we can force lists into strings by applying the list `.join` method. Let's use it to turn the first entry of our `trp_lemmas` list into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "executionInfo": {
     "elapsed": 2746,
     "status": "ok",
     "timestamp": 1594912484749,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "be-Xgn3n2qlJ",
    "outputId": "d6b2a39f-e289-4281-d8f9-8f6543812d03"
   },
   "outputs": [],
   "source": [
    "trp_str = ' '.join(trp_lemmas[0])\n",
    "trp_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4fBg61yGZFXF"
   },
   "source": [
    "## Putting it all together\n",
    "After all that, you should be well-equipped to understand this preprocessing function. It takes a DataFrame in, removes the empty values, then removes punctuation, tokenizes and lemmatizes the selftext. It then spits the text back out as a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K6Pr-0LEZFXG"
   },
   "outputs": [],
   "source": [
    "def preprocessing(df):\n",
    "    \"\"\"POS tags and filters DF by nouns\"\"\"\n",
    "    dfLength = len(df)\n",
    "    total = \"\"\n",
    "    counter = 0\n",
    "    clean = df[~df['selftext'].isin(['[removed]', '[deleted]' ])].dropna(subset=['selftext'])\n",
    "    for text in clean['selftext']:\n",
    "        # turn to lowercase\n",
    "        text = text.lower()\n",
    "        # remove punctuation\n",
    "        text = ''.join(ch for ch in text if ch not in string.punctuation)\n",
    "        # tokenize\n",
    "        tokens = word_tokenize(text)\n",
    "        # lemmatize\n",
    "        lemmas = ' '.join([wordnet_lemmatizer.lemmatize(token) for token in tokens])\n",
    "        # save\n",
    "        total += lemmas\n",
    "        counter += 1\n",
    "        if counter % 100 == 0:\n",
    "            print(\"Saved \" + str(counter) + \" out of \" + str(dfLength) + \" entries\") \n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aZonK-EC4kcJ"
   },
   "source": [
    "Let's run our function on the first 1000 entries of our DataFrames (just to save some time)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "executionInfo": {
     "elapsed": 10640,
     "status": "ok",
     "timestamp": 1594912501110,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "C0ibFc_DZFXI",
    "outputId": "1cad0505-d808-4c6d-d6d0-b4dc8caba42b",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "trp_pp = preprocessing(trp[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "executionInfo": {
     "elapsed": 13564,
     "status": "ok",
     "timestamp": 1594912505174,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "qbdNoIMNZFXK",
    "outputId": "f32de9b5-adf2-4c6a-f8c9-cb84c1e5bade"
   },
   "outputs": [],
   "source": [
    "sed_pp = preprocessing(sed[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 187
    },
    "executionInfo": {
     "elapsed": 16322,
     "status": "ok",
     "timestamp": 1594912508334,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "Hr86_R4xZFXP",
    "outputId": "678047f6-5c41-4a1a-b03f-8d1de2848660",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mgtow_pp = preprocessing(mgtow[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trp_pp[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EpWynSoy4t8o"
   },
   "source": [
    "## Tf-idf\n",
    "Tf–idf or TFIDF, short for *term frequency–inverse document frequenc*y, is a numerical statistic that reflects how important a word is to a document in a collection or corpus.\n",
    "Tf_idf is often used as a weighting factor in searches of information retrieval, text mining, and user modeling. The tf–idf value increases proportionally to the number of times a word appears in the document (the term frequency, or tf), and is offset by the number of documents in the corpus that contain the word (the inverse document frequency, or idf). This helps to adjust for the fact that some words appear more frequently in general – such as articles and prepositions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yvxZ-5f6ZFWe"
   },
   "source": [
    "### Testing tf-idf with a toy dataset\n",
    "\n",
    "Let's try tf-idf out with a toy dataset. Here we have three documents about Python, but with different meanings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MjjWn-phZFWf"
   },
   "outputs": [],
   "source": [
    "document1 = \"\"\"Python is a 2000 made-for-TV horror movie directed by Richard\n",
    "Clabaugh. The film features several cult favorite actors, including William\n",
    "Zabka of The Karate Kid fame, Wil Wheaton, Casper Van Dien, Jenny McCarthy,\n",
    "Keith Coogan, Robert Englund (best known for his role as Freddy Krueger in the\n",
    "A Nightmare on Elm Street series of films), Dana Barron, David Bowe, and Sean\n",
    "Whalen.\"\"\"\n",
    "\n",
    "document2 = \"\"\"Python, from the Greek word (πύθων/πύθωνας), is a genus of\n",
    "nonvenomous pythons[2] found in Africa and Asia. Currently, 7 species are\n",
    "recognised.[2] A member of this genus, P. reticulatus, is among the longest\n",
    "snakes known.\"\"\"\n",
    "\n",
    "document3 = \"\"\"Monty Python (also collectively known as the Pythons) are a British \n",
    "surreal comedy group who created the sketch comedy television show Monty Python's \n",
    "Flying Circus, which first aired on the BBC in 1969. Forty-five episodes were made \n",
    "over four series.\"\"\"\n",
    "\n",
    "document4 = \"\"\"Python is an interpreted, high-level, general-purpose programming language. \n",
    "Created by Guido van Rossum and first released in 1991, Python's design philosophy emphasizes \n",
    "code readability with its notable use of significant whitespace. Its language constructs and \n",
    "object-oriented approach aim to help programmers write clear, logical code for small and \n",
    "large-scale projects.\"\"\"\n",
    "\n",
    "document5 = \"\"\"The Colt Python is a .357 Magnum caliber revolver formerly\n",
    "manufactured by Colt's Manufacturing Company of Hartford, Connecticut.\n",
    "It is sometimes referred to as a \"Combat Magnum\".[1] It was first introduced\n",
    "in 1955, the same year as Smith &amp; Wesson's M29 .44 Magnum. The now discontinued\n",
    "Colt Python targeted the premium revolver market segment.\"\"\"\n",
    "\n",
    "document6 = \"\"\"The Pythonidae, commonly known simply as pythons, from the Greek word python \n",
    "(πυθων), are a family of nonvenomous snakes found in Africa, Asia, and Australia. \n",
    "Among its members are some of the largest snakes in the world. Eight genera and 31\n",
    "species are currently recognized.\"\"\"\n",
    "\n",
    "test_list = [document1, document2, document3, document4, document5, document6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iVLHjD29ZFWi"
   },
   "source": [
    "We will be using Scikit-LEARN `TfidfVectorizer`. It is a class that basically allows us to create a matrix of word counts, and immediately transform them into tf-idf values. See [here](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html) for the documentation if you want to learn more.\n",
    "\n",
    "In the second line below, we instantiate an object of the vectorizer. Then, we run it by applying the `fit_transform` method to our `test_list`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WeTAynM9xxxr"
   },
   "outputs": [],
   "source": [
    "# settings that you use for count vectorizer will go here\n",
    "tfidf_vectorizer = TfidfVectorizer(max_df=0.85, decode_error='ignore', stop_words='english',smooth_idf=True,use_idf=True)\n",
    "\n",
    "# fit and transform the texts\n",
    "tfidf_vectorizer_vectors = tfidf_vectorizer.fit_transform(test_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Uoj5xOi8vu_"
   },
   "source": [
    "Let's have a peek at our matrix by running the `.toarray()` method. \n",
    "This shows us one value per word in the total vocabulary.\n",
    "\n",
    "Notice that we're printing the vectors at index 2: this shows us the tf-idf features of all the words in `document3` (due to zero-based indexing). All the empty values are simply words that are not present in `document3`!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 527
    },
    "executionInfo": {
     "elapsed": 12411,
     "status": "ok",
     "timestamp": 1594912508338,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "gRjP38dV-7I7",
    "outputId": "95f88be5-a1d1-4573-e5db-db14c82ad9f5"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer_vectors.toarray()[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XGuoetn0ASpf"
   },
   "source": [
    "We can also have a look at some of the words in our total vocabulary by running `get_feature_names()` method. Let's grab a few."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 10611,
     "status": "ok",
     "timestamp": 1594912508339,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "hWPqOTR_AIM0",
    "outputId": "258adeac-8e19-4e32-f702-63d45e690fd2"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer.get_feature_names()[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1iydB28vAgEt"
   },
   "source": [
    "As you can see, the second word is '1969', which as you can see in the printout of our `.toarray()` is a distinctive word for Monty Python (the first airing of their TV show)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XJjS79P6w2Y"
   },
   "source": [
    "### Putting distinctive words in a DataFrame\n",
    "We can now take out one vector (i.e., the tf-idf values of one text) that `.fit_transform()` yielded. We can put them in a DataFrame, and print out that DataFrame after sorting it based on the highest score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "executionInfo": {
     "elapsed": 8946,
     "status": "ok",
     "timestamp": 1594912508340,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "1fpzFSP6x9es",
    "outputId": "8d92afe6-4e30-43af-cfeb-3501afbfd80e"
   },
   "outputs": [],
   "source": [
    "# get the vector for the third document\n",
    "vector_tfidfvectorizer = tfidf_vectorizer_vectors[2] # Note that 2 refers to document3, due to zero-based indexing\n",
    "\n",
    "# place tf-idf values in a DataFrame\n",
    "df = pd.DataFrame(vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s9qVYEm_ZFW2"
   },
   "source": [
    "Looks like it works! Through tf-idf, we have found the words that are most-distinctive of `document3`!\n",
    "\n",
    "Note that we have used `Tfidfvectorizer` here, which internally computes word counts, IDF values, and tf-idf scores for our dataset. If you only want to use the term frequency (term count) vectors for different tasks, you have to use `Tfidftransformer`. See e.g. [here](https://kavita-ganesan.com/tfidftransformer-tfidfvectorizer-usage-differences/#.XviBUJMzbzg) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mq3QvXEcZFXS"
   },
   "source": [
    "## Using tf-idf on Reddit datasets\n",
    "Tf-idf is a basic but intuitive way to find words that are typical of a particular subreddit, when compared to other comparable subreddits.\n",
    "\n",
    "We'll implement scikit-learn's tf-idf functionality to find distinctive words for each document (i.e. subreddit). So we'll treat each subreddit we feed into the `TfidfTransformer` as a document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aSQHlQliZFXV"
   },
   "outputs": [],
   "source": [
    "reddit_list = [trp_pp, sed_pp, mgtow_pp]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgKvuY96ZFXY"
   },
   "source": [
    "Your turn! Using `TfidfVectorizer`, repeat what we did with our test data, but this time with the `reddit_list`! It's exactly the same procedure – only the name of the list changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 979
    },
    "executionInfo": {
     "elapsed": 7486,
     "status": "ok",
     "timestamp": 1594912509862,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "1vRKbLpDZFXY",
    "outputId": "c6fef75b-7f3f-48c6-c676-6e43ae1f3295"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_DAxZynUCF8L"
   },
   "source": [
    "As you can see, there are still some HTML artifacts left, such as \"nbsp\". At a later point we'll look at how to remove these annoying tokens. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_YoFv8pXZFXa"
   },
   "source": [
    "## Bonus: Using TF-IDF to find similar documents\n",
    "*Note: the below code is a bit more advanced, and for demonstration purposes. Don't worry if you don't fully get it!*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qjvztHrBZFXb"
   },
   "source": [
    "We can also use tf-idf to work out the similarity between any pair of documents. So given one post or comment, we could see which posts or comments are most similar. This can be useful if you're trying to find other examples of a pattern you have found and want to explore further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aluiuMKvZFXb"
   },
   "source": [
    "This time, our \"documents\" will not be entire subreddits, but posts/submissions within one subreddit. Let's import the submissions and run the vectorizer without the preprocessing and lemmatizing. Tf-idf will still work this way, and this way, we will be able to read our posts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "04hxWFEXZFXc"
   },
   "outputs": [],
   "source": [
    "tfidf_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1,3), min_df = 0, stop_words = 'english')\n",
    "word_count_vectors = tfidf_vectorizer.fit_transform([post for post in trp['selftext']])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "px4XE13U4Wwb"
   },
   "source": [
    "We'll start by finding a post with a clear topic. Let's grab the 1st entry in our dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "executionInfo": {
     "elapsed": 51448,
     "status": "ok",
     "timestamp": 1594912557189,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "6n1IDR9y4Wwc",
    "outputId": "6c2419dd-511b-4b72-cb6a-40fc262db759"
   },
   "outputs": [],
   "source": [
    "trp['selftext'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6l6lqgF4B0L"
   },
   "source": [
    "This one seems to be about Joe Rogan, an American celebrity. Let's have a quick look at the tfidf scores for the words in this submission to see if his name is indeed typical for this particular submission:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 359
    },
    "executionInfo": {
     "elapsed": 66937,
     "status": "ok",
     "timestamp": 1594912573664,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "rpOde4Sr4AzJ",
    "outputId": "5f850a60-455e-4d7d-b099-fd6763d9186d"
   },
   "outputs": [],
   "source": [
    "# get a vector out\n",
    "vector_tfidfvectorizer = word_count_vectors[0] # change this number if you want to pick out a different vector / text\n",
    "\n",
    "# place tf-idf values in a pandas data frame\n",
    "df = pd.DataFrame(vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\n",
    "df.sort_values(by=[\"tfidf\"],ascending=False)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kiTzCg4xZFXi"
   },
   "source": [
    "Looks like it is.\n",
    "\n",
    "Now let's find some similar document(s). \n",
    "The fact that our documents are now in a vector space is convenient: it allows us to make use of mathematical similarity metrics.\n",
    "\n",
    "**Cosine similarity** is one metric used to measure how similar the documents are irrespective of their size. Mathematically, it measures the cosine of the angle between two vectors projected in a multi-dimensional space.\n",
    "\n",
    "Don't worry too much about this function for now: just run it and let's see how it works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rbfy9VoCEiPc"
   },
   "outputs": [],
   "source": [
    "def find_similar(word_count_vectors, index, top_n = 5):   # you can change the `top_n` parameter if you want to retrieve more similar documents\n",
    "    cosine_similarities = linear_kernel(word_count_vectors[index:index+1], word_count_vectors).flatten()\n",
    "    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n",
    "    return [(index, cosine_similarities[index]) for index in related_docs_indices][0:top_n]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuATLPrFZFXe"
   },
   "source": [
    "The above function finds similar words. It uses scikit-LEARN's `linear kernel`, which uses cosine similarity to find documents that are most alike.\n",
    "\n",
    "We can now throw the resulting scores and similar posts in a list, feed that list into a DataFrame, and check out one of them to see if it works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 64739,
     "status": "ok",
     "timestamp": 1594912574429,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "6MuqmwKBZFXk",
    "outputId": "1705ac57-7b4e-416d-f20b-afcae91dde03"
   },
   "outputs": [],
   "source": [
    "cosine = []\n",
    "for index, score in find_similar(word_count_vectors, 0):\n",
    "  cosine.append(\n",
    "      {'cos_score': score, \n",
    "       'text': trp['selftext'][index]\n",
    "       }\n",
    "  )\n",
    "cosine_df = pd.DataFrame(cosine)\n",
    "cosine_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "executionInfo": {
     "elapsed": 63879,
     "status": "ok",
     "timestamp": 1594912574429,
     "user": {
      "displayName": "Tom van Nuenen",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhtEyfMMXnweSdKHvOqheyZd6KLkXGOtz3AYYAIBA=s64",
      "userId": "10012302451096885058"
     },
     "user_tz": -180
    },
    "id": "-dEbe-PoIizd",
    "outputId": "f2a6b7aa-dc1e-46e9-dbbe-9c1dd9f5c7c7"
   },
   "outputs": [],
   "source": [
    "cosine_df['text'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r16ecNp45gWB"
   },
   "source": [
    "This post does seem comparable! It's also about Joe Rogan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPcdL8H6ZFXm"
   },
   "source": [
    "## Reflection: hypothesis generation using tf-idf\n",
    "\n",
    "Think about a hypothesis or research question you could construct about your own subreddit based on these distinctive words and related posts."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "pXhTdS5Ph2ND",
    "pn6giFWubji1",
    "22GAJ_PQnjT-",
    "6rNmrlsxsGbB",
    "4fBg61yGZFXF",
    "EpWynSoy4t8o",
    "Mq3QvXEcZFXS",
    "_YoFv8pXZFXa"
   ],
   "name": "2 Preprocessing & tf-idf.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
