{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5K5Zn8qobjic"
   },
   "source": [
    "# SISU Digital Humanities: Textual and Language Analysis on Social Media<br />\n",
    "### Distant Reading using NLTK and Pandas\n",
    "Created by Tom van Nuenen (tom.van_nuenen@kcl.ac.uk) <br />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gJHDHrQPbjif"
   },
   "source": [
    "# Distant Reading\n",
    "\n",
    "This notebook focuses on methods to engage in a basic distant reading using NLTK. It also reiterates some basic Pandas operations from the previous notebook. By the end of this notebook, you will:\n",
    "\n",
    "* Know how to open and perform simple operations on a DataFrame;\n",
    "* Use NLTK's `Text()` object to perform some basic distant reading operations on a subreddit.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLTXX5Dtbjii"
   },
   "source": [
    "## Importing packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aBB4BWsPbjij"
   },
   "source": [
    "Let's start by importing some packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "id": "Rw1SGwFsbjik",
    "outputId": "86c1c58f-a5c1-46cd-b42f-1cc9d39b2e98"
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.book import FreqDist\n",
    "from nltk.text import Text\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import collections\n",
    "import string\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['figure.figsize'] = [10, 5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QuIFFo8clcwo"
   },
   "source": [
    "## Pandas basics & Working with Reddit data\n",
    "\n",
    "Let's get the data, which is taken from the MOOC Intercultural Communication (https://centerforinterculturaldialogue.org/2021/03/17/mooc-intercultural-communication-2021-china/). \n",
    "\n",
    "Using the `.head()` method we can get the first n rows of a df. The default is 5. We can add a *parameter* (here 3) to indicate how many rows we want to print."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/icc3-comments.csv\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "zipTYx8rlbOo",
    "outputId": "cd023806-bdc6-4001-b184-187251817377",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9za2bgknrbRx"
   },
   "source": [
    "Here's what we're seeing. Pay special attention to the \"NaN\" labels, indicating missing values. \n",
    "\n",
    "This particular dataset includes the comments on the MOOC under the \"text\" column, which is what we're most interested in.\n",
    "\n",
    "other columns contain valuable metadata you can use in your analyses, such as the \"likes\" column (i.e., the amount of likes a post has received), the \"timestamp\" column which indicates when the post was written, and the \"step\" column which refers to pedagogically organised \"Learning Steps\" focusing on particular activities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xlu74LCjjNhm"
   },
   "source": [
    "### Sorting a DF\n",
    "Using the `.sort_values()` method we can sort the df by particular columms. We use two parameters: the `by` parameter indicates by which column we want to sort, the `ascending` parameter indicated whether our sortation is in ascending or descending order.\n",
    "\n",
    "Here, I'm assigning my sorted DataFrame to the same variable `df`, effectively overwriting the old version. These are the top 10 comments based on the amount of \"likes\" they have received!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hcpxg8ezjNxA"
   },
   "outputs": [],
   "source": [
    "df = df.sort_values(by=['score'], ascending=False)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P9-L4LR4lDlz"
   },
   "source": [
    "### Selecting a column\n",
    "To select a single column of data, simply put the name of the column in between brackets. Let’s select the 'text' column. We can print out the first entry in this column as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 137
    },
    "id": "4hmKORvYlFN-",
    "outputId": "0882546c-d44e-45e8-d713-09181078cc39"
   },
   "outputs": [],
   "source": [
    "df.body[17626]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ppsb8JK9tt4j"
   },
   "source": [
    "As you see, using the `[]` operator selects a set of rows and/or columns from a DataFrame.\n",
    "\n",
    "Your turn! Use slicing to retrieve the first 10 \"selftext\" entries in our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "xjxlNNK_syRD",
    "outputId": "36358a18-9ed3-434a-cd11-241f196142ee"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "df['text'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EWrEqa9rvL8U"
   },
   "source": [
    "One thing we often do when we’re exploring a dataset is filtering the data based on a given condition. For example, we might need to find all the rows in our dataset that have received more than 10 \"likes\". We can use the `.loc[]` method to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "id": "ZrlTcUzKvK_Y",
    "outputId": "0e112175-e2f6-4b94-d2e3-fe2c52b4d49a"
   },
   "outputs": [],
   "source": [
    "df.loc[df.score >= 10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ym6Q_twP5E2y"
   },
   "source": [
    "`.loc[]` is a powerful method that can be used for all kinds of research purposes, if you want to filter or prune your dataset based on some condition. For more info, see [here](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.loc.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SMglX5TU0J5F"
   },
   "source": [
    "Your turn! Use `loc[]` to retrieve only the posts that have over 200 comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_dVKTaN8LutK"
   },
   "source": [
    "### Removing rows\n",
    "Missing values (`NaN`) in a DataFrame can cause a lot of errors. In general, it's a god idea to get rid of those rows whose \"selftext\" is missing. This works as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 164
    },
    "id": "6khNIe0kLvOf",
    "outputId": "6e8db553-e9fe-403c-89c5-67d7375d6d3e"
   },
   "outputs": [],
   "source": [
    "print(\"length of df is now \" + str(len(df)))\n",
    "clean_df = df.dropna(subset=['text'])\n",
    "print(\"length of df is now \" + str(len(clean_df)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lSdM51N2bjkc"
   },
   "source": [
    "## Distant reading with NLTK \n",
    "Let's use NLTK's `word_tokenize()` method. We've imported this library at the beginning of this notebook.\n",
    "`word_tokenize()` works like this:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 340
    },
    "id": "NzuuVD23-QNM",
    "outputId": "4592b966-bc58-4ed2-d9bf-ce68937b9e1b"
   },
   "outputs": [],
   "source": [
    "word_tokenize(\"He is a lumberjack and he is okay. He sleeps all night and he works all day.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OscceKEtbjkd"
   },
   "source": [
    "Your turn! Let's tokenize our \"selftext\" column. Here's what you need to do: \n",
    "- Create a new list called `sed_tokens`;\n",
    "- Begin a for-loop that iterates over the \"selftext\" column of our `sed` DataFrame; \n",
    "- `For` each text in that column, tokenize it using `word_tokenize()`; \n",
    "- Add these tokenized words to our new `sed_tokens` list using the list `.extend()` method*.\n",
    " \n",
    "*We use `.extend` instead of `.append`. This is because we want one long list, instead of a list of lists. While `append` adds its argument as a single element to the end of a list – meaning the length of the list itself will increase by one – `extend` adds each element to the list, extending the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqrTrJ33bjkd"
   },
   "outputs": [],
   "source": [
    "# Your code here\n",
    "\n",
    "df_tokens = []\n",
    "\n",
    "for text in df['body']:\n",
    "    df_tokens.extend(word_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yK1DCuAJbjkh"
   },
   "source": [
    "## The NLTK `Text()` class\n",
    "Now, let's have a look at our data. NLTK provides a `Text()` class, which is a \"wrapper\" that allows for inital exploration of texts. It supports counting, concordancing, collocation discovery, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "58NEJVb2bjki"
   },
   "outputs": [],
   "source": [
    "# Here, we create our NLTK Text object\n",
    "df_t = Text(df_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAdI-HgMbjkl"
   },
   "source": [
    "Let's print out the \"docstring\" of NLTK's `Text()` object, as well as all the things you can do with this object. Have a read through this to see what it allows you to do!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "GvexG4e7bjkm",
    "outputId": "9ec603e7-f54f-4b96-fe6d-8dddf172671e",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "help(Text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word counts\n",
    "Let's run a few of these functions. How often do people talk about a \"problem\"? Let's find out using the `.count()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.count('problem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TPatGDHDOpaU"
   },
   "source": [
    "### Concordances \n",
    "One of the most basic, but quite helpful, ways to quickly get an overview of the contexts in which a word appears is through a `.concordance()` view. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 459
    },
    "id": "nkDsXJkPbjkp",
    "outputId": "938e2d30-8361-429e-8f5e-05f79add673b",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_t.concordance('problem', width=115)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collocations\n",
    "\n",
    "A collocation is a sequence of words that often appear together. The `.collocations()` method can find these in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_t.collocations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Np8M59BeDJll"
   },
   "source": [
    "### Word plotting\n",
    "Using the `.dispersion_plot()` method we can easily visualize how often some word appears throughout the text. We have to feed it a list with several words in it.\n",
    "\n",
    "Sorting our df by date allows us to look \"through time\" to see whether particular words start (dis)appearing in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 295
    },
    "id": "_myUpUcqDIdh",
    "outputId": "3bfec8cb-f870-4b19-f945-82dc2843f5d5"
   },
   "outputs": [],
   "source": [
    "df_t.dispersion_plot([\"problem\", \"issue\", \"worry\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d5q_G1GKkZSy"
   },
   "source": [
    "### Similar words\n",
    "Using the `.similar()` method we can look at \"distributional similarity\": finding other words which appear in the same contexts as the specified word.\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "id": "mWhoVtkrjBx_",
    "outputId": "bacd5a57-dca8-44fd-9878-43b9bd89150a"
   },
   "outputs": [],
   "source": [
    "df_t.similar('problem')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common context\n",
    "The `.common_contexts()` method allows us to study the common context of two or more words. We must enclose these words in square brackets and round brackets, separated by commas. For instance, when people talk about problems, how often do they mention the word \"communication\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_t.common_contexts(['problem', 'job'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fdist = FreqDist(df_t)\n",
    "fdist.most_common(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can visualize these numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.plot(50, cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's have a look at the top-10 words whose length is greater than 6 and whose word frequency is greater than 1500."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(w for w in set(df_t) if len(w)>6 and fdist[w]>1500)[:10]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Y-CM9cU_bjky"
   ],
   "name": "Week 2-3 Reddit Distant reading ANSWERS.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
